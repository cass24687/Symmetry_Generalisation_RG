{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLT\n",
    "\n",
    "This script correctly find the weights such that the network learns the deterministic map $\\zeta_{n+1} = \\frac{1}{\\sqrt{2}} (\\zeta_n^1 + \\zeta_n^2)$. Let's choose to consider the uniform distribution, and use MSE loss.\n",
    "\n",
    "We attempt to find the network mapping of cumulants ('the statistical model') in this Gaussian approximation, and compare it to the analytical 'true' cumulant transformation that we know that the CLT RG decimation induces. Cumulants should transform individually in a way that is consistent with Fischer paper's framework under these weights. Where nonlinearities are used, this notebook uses the Fischer approach of only considering the network as propagating Gaussian statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 43\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "Here we consider a uniform distribution.\n",
    "\n",
    "First define some useful functions to calculate cumulants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulant_extraction(data):\n",
    "    \"\"\"\n",
    "    Compute the first four cumulants for a 1D array.\n",
    "    Parameters:\n",
    "        data: np.ndarray of shape (N,)\n",
    "    Returns:\n",
    "        A dictionary with cumulants k1 to k4\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)                          # κ1\n",
    "    centered = data - mean\n",
    "\n",
    "    var = np.mean(centered**2)                    # κ2\n",
    "    third = np.mean(centered**3)                  # κ3\n",
    "    fourth_raw = np.mean(centered**4)\n",
    "    fourth = fourth_raw - 3 * var**2              # κ4\n",
    "\n",
    "    return {\n",
    "        'kappa1': mean,\n",
    "        'kappa2': var,\n",
    "        'kappa3': third,\n",
    "        'kappa4': fourth\n",
    "    }\n",
    "\n",
    "def expected_cumulant_evolution(data):\n",
    "    \"\"\"Input: data is a dictionary of input cumulants\n",
    "    This function scales each cumulant according to expected transform from the Jona-Lasinio paper.\"\"\"\n",
    "    \n",
    "    scaled_cumulants = {}\n",
    "    for key, value in data.items():\n",
    "        # Extract the cumulant order (k) from the key string 'kappa{k}'\n",
    "        k = int(key[-1])\n",
    "        \n",
    "        # Calculate the scaling factor\n",
    "        scaling_factor = 2 ** (1 - k / 2)\n",
    "        \n",
    "        # Scale the cumulant value\n",
    "        scaled_cumulants[key] = value * scaling_factor\n",
    "    \n",
    "    return scaled_cumulants\n",
    "\n",
    "import itertools\n",
    "def symmetrise_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Symmetrise a tensor over all its axes.\n",
    "    Args:\n",
    "        tensor (np.ndarray): A tensor of shape (d, d, ..., d)\n",
    "    Returns:\n",
    "        sym_tensor (np.ndarray): Symmetrised version of the input tensor.\n",
    "    \"\"\"\n",
    "    order = tensor.ndim\n",
    "    perms = list(itertools.permutations(range(order)))\n",
    "    sym_tensor = sum(np.transpose(tensor, axes=perm) for perm in perms)\n",
    "    sym_tensor /= len(perms)\n",
    "    return sym_tensor\n",
    "\n",
    "def cumulant_extraction_multidim(data):\n",
    "    \"\"\"\n",
    "    Compute the first four cumulants for a 2D array.\n",
    "    Parameters:\n",
    "        data: np.ndarray of shape (N, 2)\n",
    "    Returns:\n",
    "        A dictionary with cumulants k1 to k4\n",
    "    \"\"\"\n",
    "    N, d = data.shape\n",
    "    \n",
    "    # Mean vector (κ1)\n",
    "    mean = np.mean(data, axis=0)  # Shape: (2,)\n",
    "    \n",
    "    # Centered data\n",
    "    centered = data - mean  # Shape: (N, 2)\n",
    "    \n",
    "    # Covariance matrix (κ2)\n",
    "    \" Using einsum() to find the covariance is implicitly assuming that there is already some level of correlation\"\n",
    "    \" Between the two dimensions. If the data is uncorrelated, this will yield a covariance matrix with off-diagonal elements close to zero.\"\n",
    "    \" But the data should be two independent columns initially, with a diagonal covariance - and correlations will emerge as you propagate them.\"\n",
    "    \n",
    "    \" We shouldn't manually diagonalise the covariance.\"\n",
    "    \n",
    "    # covariance = np.einsum('ni,nj->ij', centered, centered) / N  # Shape: (2, 2)\n",
    "    # covariance = np.diag(np.var(data, axis=0))  # Shape: (2, 2)\n",
    "\n",
    "    # κ2: Covariance matrix\n",
    "    covariance = np.einsum('ni,nj->ij', centered, centered) / N  # Shape: (2, 2)\n",
    "    \n",
    "    # Third-order cumulant tensor (κ3)\n",
    "    G3_raw = np.einsum('ni,nj,nk->ijk', centered, centered, centered) / N  # Shape: (2, 2, 2)\n",
    "    G3 = symmetrise_tensor(G3_raw)\n",
    "    \n",
    "    # Fourth-order cumulant tensor (κ4)\n",
    "    G4_raw = np.einsum('ni,nj,nk,nl->ijkl', centered, centered, centered, centered) / N  # Shape: (2, 2, 2, 2)\n",
    "    \n",
    "    # # Applying the correction for the fourth cumulant\n",
    "    # kappa4 = G4_raw - 3 * np.einsum('ij,kl->ijkl', covariance, covariance) # only valid for jointly gaussian data\n",
    "    \n",
    "    # Full Gaussian correction for fourth cumulant\n",
    "    cov = covariance\n",
    "    term1 = np.einsum('ij,kl->ijkl', cov, cov)\n",
    "    term2 = np.einsum('ik,jl->ijkl', cov, cov)\n",
    "    term3 = np.einsum('il,jk->ijkl', cov, cov)\n",
    "    kappa4_raw = G4_raw - (term1 + term2 + term3)\n",
    "    kappa4 = symmetrise_tensor(kappa4_raw)  # Shape: (2, 2, 2, 2)\n",
    "    \n",
    "    # Package the results in a dictionary\n",
    "    cumulants = {\n",
    "        'kappa1': torch.tensor(mean, dtype=torch.float64),                     # Shape: (2,)\n",
    "        'kappa2': torch.tensor(covariance, dtype=torch.float64),               # Shape: (2, 2)\n",
    "        'kappa3': torch.tensor(G3, dtype=torch.float64),                       # Shape: (2, 2, 2)\n",
    "        'kappa4': torch.tensor(kappa4, dtype=torch.float64)                        # Shape: (2, 2, 2, 2)\n",
    "    }\n",
    "    \n",
    "    return cumulants\n",
    "\n",
    "# Find input cumulants:\n",
    "def find_input_cumulants(dataset):\n",
    "\n",
    "\n",
    "    # Extract inputs and labels from dataset\n",
    "    xs = np.array([x.numpy() for x, _ in dataset])  # shape (N, 2)\n",
    "    \n",
    "    # Compute column-wise cumulants\n",
    "    col1 = cumulant_extraction(xs[:, 0])\n",
    "    col2 = cumulant_extraction(xs[:, 1])\n",
    "    avg_cumulants = {k: 0.5 * (col1[k] + col2[k]) for k in col1}\n",
    "\n",
    "    # Full multivariate cumulants\n",
    "    z_dict = cumulant_extraction_multidim(xs)\n",
    "\n",
    "\n",
    "    # # Of each column:\n",
    "    # first_col = []\n",
    "    # second_col = []\n",
    "    # labels = []\n",
    "    # for i in range(len(data_samples)):\n",
    "    #     x, y = data_samples[i]\n",
    "    #     first_col.append(x[0].item())\n",
    "    #     second_col.append(x[1].item())\n",
    "    #     labels.append(y.item())\n",
    "\n",
    "    # first_col = np.array(first_col)  # Shape: (n_train, )\n",
    "    # second_col = np.array(second_col)  # Shape: (n_train, )\n",
    "    # labels = np.array(labels)  # Shape: (n_train, )\n",
    "\n",
    "    # # Find average input cumulants for each column\n",
    "    # col1 = cumulant_extraction(first_col)\n",
    "    # print(\"\\nInitial Empirical Cumulants (first col):\")\n",
    "    # for k, v in col1.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "    # col2 = cumulant_extraction(second_col)\n",
    "    # print(\"\\nInitial Empirical Cumulants (second col):\")\n",
    "    # for k, v in col2.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # # Find average input cumulants across the two columns\n",
    "    # avg_cumulants = {}\n",
    "    # for k in col1.keys():\n",
    "    #     avg_cumulants[k] = 0.5 * (col1[k] + col2[k])\n",
    "\n",
    "    # print(\"Average Empirical Cumulants (across both input columns):\")\n",
    "    # for k, v in avg_cumulants.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "    # # # Compute the cumulants for the input data (multivariate)\n",
    "    # input_array = data_samples.samples\n",
    "    # z_dict = cumulant_extraction_multidim(input_array)\n",
    "\n",
    "    # print(\"\\nCumulants of Input Data:\")\n",
    "    # for k, v in z_dict.items():\n",
    "    #     print(f\"{k}: shape = {v.shape}, value = {v}\")\n",
    "\n",
    "    return avg_cumulants, z_dict\n",
    "\n",
    "# Compute output cumulants\n",
    "def find_pred_output_cumulants(dataset):\n",
    "\n",
    "    #  # Of each column:\n",
    "    # first_col = []\n",
    "    # second_col = []\n",
    "    # labels = []\n",
    "    # for i in range(len(data_samples)):\n",
    "    #     x, y = data_samples[i]\n",
    "    #     first_col.append(x[0].item())\n",
    "    #     second_col.append(x[1].item())\n",
    "    #     labels.append(y.item())\n",
    "\n",
    "    # first_col = np.array(first_col)  # Shape: (n_train, )\n",
    "    # second_col = np.array(second_col)  # Shape: (n_train, )\n",
    "    # labels = np.array(labels)  # Shape: (n_train, )\n",
    "\n",
    "    # # Jona-Lasinio transform, averaged\n",
    "    # col1 = cumulant_extraction(first_col)\n",
    "    # col2 = cumulant_extraction(second_col)\n",
    "    # expected_cumulants_col1 = expected_cumulant_evolution(col1)\n",
    "    # expected_cumulants_col2 = expected_cumulant_evolution(col2)\n",
    "    # avg_evolved_cumulants = {}\n",
    "    # for k in expected_cumulants_col1.keys():\n",
    "    #     avg_evolved_cumulants[k] = 0.5 * (expected_cumulants_col1[k] + expected_cumulants_col2[k])\n",
    "\n",
    "    # print(\"\\nAverage expected evolved cumulants (Jona-Lasinio), across both columns:\\n\")\n",
    "    # for k, v in avg_evolved_cumulants.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "    # # Ground truth labels\n",
    "    # ground_truth_cumulants = cumulant_extraction(labels)\n",
    "    # print(\"\\nCumulants of the ground truth labels:\")\n",
    "    # for k, v in ground_truth_cumulants.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "    # print(\"-------------------\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    xs = np.array([x.numpy() for x, _ in dataset])\n",
    "    ys = np.array([y.item() for _, y in dataset])\n",
    "\n",
    "    col1 = cumulant_extraction(xs[:, 0])\n",
    "    col2 = cumulant_extraction(xs[:, 1])\n",
    "\n",
    "    evolved1 = expected_cumulant_evolution(col1)\n",
    "    evolved2 = expected_cumulant_evolution(col2)\n",
    "    avg_evolved_cumulants = {k: 0.5 * (evolved1[k] + evolved2[k]) for k in evolved1}\n",
    "\n",
    "    ground_truth_cumulants = cumulant_extraction(ys)\n",
    "\n",
    "    return avg_evolved_cumulants, ground_truth_cumulants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section is now defunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample pairs of points from a 1D uniform distribution\n",
    "# def adjust_samples_to_match_cumulants(y, target_cumulants):\n",
    "#     \"\"\"Adjust distribution to match desired cumulants (preserve mean).\"\"\"\n",
    "#     y_centered = y - np.mean(y)\n",
    "#     y_scaled = y_centered / np.std(y)\n",
    "\n",
    "#     # Use Gram-Charlier-like polynomial correction (not perfect, but practical)\n",
    "#     skew_target = target_cumulants['kappa3'] / (target_cumulants['kappa2'] ** 1.5)\n",
    "#     kurt_target = target_cumulants['kappa4'] / (target_cumulants['kappa2'] ** 2)\n",
    "\n",
    "\n",
    "#     # Fit using a 3rd/4th order polynomial transform to match shape (approximate)\n",
    "#     corrected = y_scaled \\\n",
    "#         + 0.5 * skew_target * (y_scaled**2 - 1) \\\n",
    "#         + (1/24) * kurt_target * (y_scaled**3 - 3*y_scaled)\n",
    "\n",
    "#     # Rescale to target mean and std\n",
    "#     corrected = corrected * np.sqrt(target_cumulants['kappa2']) + target_cumulants['kappa1']\n",
    "#     return corrected\n",
    "\n",
    "# def get_data(number):\n",
    "#     x = np.random.uniform(low=-0.5, high=0.5, size=(number, 2))\n",
    "#     decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "\n",
    "#     first_col = x[:, 0]\n",
    "#     second_col = x[:, 1]\n",
    "\n",
    "#     # Find input cumulants\n",
    "#     col1 = cumulant_extraction(first_col)\n",
    "#     col2 = cumulant_extraction(second_col)\n",
    "#     avg_cumulants = {}\n",
    "#     for k in col1.keys():\n",
    "#         avg_cumulants[k] = 0.5 * (col1[k] + col2[k])\n",
    "#     expected_y_cumulants = expected_cumulant_evolution(avg_cumulants)\n",
    "\n",
    "#     decimated_x_corrected = adjust_samples_to_match_cumulants(decimated_x, expected_y_cumulants)\n",
    "#     return x, decimated_x_corrected\n",
    "\n",
    "# # Check\n",
    "# test_in, test_out = get_data(1000)\n",
    "# for i in range(len(test_in[:, 0])):\n",
    "#     if test_out[i] != np.sum(test_in[i]) / np.sqrt(2):\n",
    "#         print(\"Error in cumulant matching\")\n",
    "#         break\n",
    "\n",
    "# test_out_cumulants = cumulant_extraction(test_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data - Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pairs of points from a 1D uniform distribution\n",
    "def get_data_uniform(number):\n",
    "    x = np.random.uniform(low=0, high=1.0, size=(number, 2))\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "def get_data_gaussian(number): # centred unit gaussian\n",
    "    x = np.random.normal(loc=0, scale=1.0, size=(number, 2)) # loc is mean, scale is stdev (not variance)\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "def get_data_gaussian_shifted(number): # unit gaussian, mean shifted to 1/sqrt(2)\n",
    "    x = np.random.normal(loc=1/(np.sqrt(2)), scale=1.0, size=(number, 2)) # loc is mean, scale is stdev (not variance)\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "def get_data_gaussian_varied(number, var):\n",
    "    x = np.random.normal(loc=0, scale=np.sqrt(var), size=(number, 2)) # loc is mean, scale is stdev (not variance)\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "class gaussian_data(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.samples, self.true_samples = get_data_gaussian(n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "class gaussian_data_vary_std(Dataset):\n",
    "    def __init__(self, n_samples, var):\n",
    "        self.samples, self.true_samples = get_data_gaussian_varied(n_samples, var)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "class gaussian_data_shifted_mean(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.samples, self.true_samples = get_data_gaussian_shifted(n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "class uniform_data(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.samples, self.true_samples = get_data_uniform(n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "n_train = int(1e6)\n",
    "n_test = int(1e6)\n",
    "var = 8\n",
    "\n",
    "train_samples = gaussian_data(n_train)\n",
    "trainloader = DataLoader(train_samples, batch_size=64, shuffle=True)\n",
    "test_samples = gaussian_data_vary_std(n_test, var)\n",
    "testloader = DataLoader(test_samples, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding statistical source of prediction errors// Identifying reasonable sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrepancy between ground truth and JL expected scaled cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes = [10**3, 5*10**3, 10**4, 5*10**4, 10**5, 5*10**5, 10**6, 5*10**6, 10**7]\n",
    "# n_trials = 5  # number of seeds\n",
    "\n",
    "\n",
    "# var = 1\n",
    "# errors_kappa4 = []\n",
    "\n",
    "# for N in sample_sizes:\n",
    "#     deviations = []\n",
    "#     for _ in range(n_trials):\n",
    "\n",
    "#         test_samples = gaussian_data_vary_std(N, var)\n",
    "#         test_inputs, _ = find_input_cumulants(test_samples)\n",
    "#         JL_pred_cumulants, ground_truth_cumulants = find_pred_output_cumulants(test_samples)\n",
    "\n",
    "#         # Compare — e.g., deviation of κ₄\n",
    "#         k4_pred = JL_pred_cumulants['kappa4']\n",
    "#         k4_gt = ground_truth_cumulants['kappa4']\n",
    "#         # deviation = np.linalg.norm(k4_pred - k4_gt)\n",
    "#         deviation = np.abs(k4_pred - k4_gt)\n",
    "#         deviations.append(deviation)\n",
    "\n",
    "#     # Store average deviation for this N\n",
    "#     errors_kappa4.append(np.mean(deviations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(sample_sizes, errors_kappa4, marker='o', label='empirical deviations')\n",
    "# expected = [np.sqrt(24 / N) for N in sample_sizes]\n",
    "# plt.plot(sample_sizes, expected, '--', label=\"√(24/N)\")\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Sample size (N)\")\n",
    "# plt.ylabel(\"Mean deviation in κ₄\")\n",
    "# plt.title(\"Discrepancy vs. Sample Size\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# slope, intercept = np.polyfit(np.log10(sample_sizes), np.log10(errors_kappa4), 1)\n",
    "# print(\"Estimated convergence rate:\", slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Combine sample sizes and corresponding errors into rows\n",
    "# rows = zip(sample_sizes, errors_kappa4)\n",
    "\n",
    "# # Specify output file name\n",
    "# output_file = \"kappa4_samplesize_gaussian.csv\"\n",
    "\n",
    "# # Write to CSV with UTF-8 encoding\n",
    "# with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Sample Size (N)\", \"Mean Deviation in κ4\"])  # Header\n",
    "#     for row in rows:\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"Saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Diagonal L2 Norm Decay for higher order cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes = [10**3, 5*10**3, 10**4, 5*10**4, 10**5, 5*10**5, 10**6, 5*10**6, 10**7]\n",
    "# n_trials = 5  # number of seeds\n",
    "\n",
    "# # Per-sample-size trial data\n",
    "# trial_data_k2 = []\n",
    "# trial_data_k3 = []\n",
    "# trial_data_k4 = []\n",
    "\n",
    "# # Per-sample-size mean (for main trend line)\n",
    "# mean_offdiag_k2 = []\n",
    "# mean_offdiag_k3 = []\n",
    "# mean_offdiag_k4 = []\n",
    "\n",
    "# for N in sample_sizes:\n",
    "#     deviations_k2, deviations_k3, deviations_k4 = [], [], []\n",
    "\n",
    "#     for _ in range(n_trials):\n",
    "#         test_samples = gaussian_data_vary_std(N, var)\n",
    "#         cumulants = cumulant_extraction_multidim(test_samples.samples)\n",
    "\n",
    "#         # κ2: remove diagonal\n",
    "#         k2 = cumulants['kappa2'].numpy()\n",
    "#         off_k2 = k2 - np.diag(np.diag(k2))\n",
    "#         deviations_k2.append(np.sum(off_k2**2))\n",
    "\n",
    "#         # κ3: mask fully diagonal terms (i=j=k)\n",
    "#         k3 = cumulants['kappa3'].numpy()\n",
    "#         mask3 = np.ones_like(k3, dtype=bool)\n",
    "#         for i in range(k3.shape[0]):\n",
    "#             mask3[i, i, i] = False\n",
    "#         deviations_k3.append(np.sum(k3[mask3]**2))\n",
    "\n",
    "#         # κ4: mask fully diagonal terms (i=j=k=l)\n",
    "#         k4 = cumulants['kappa4'].numpy()\n",
    "#         mask4 = np.ones_like(k4, dtype=bool)\n",
    "#         for i in range(k4.shape[0]):\n",
    "#             mask4[i, i, i, i] = False\n",
    "#         deviations_k4.append(np.sum(k4[mask4]**2))\n",
    "\n",
    "#     # Save per-trial data\n",
    "#     trial_data_k2.append(deviations_k2)\n",
    "#     trial_data_k3.append(deviations_k3)\n",
    "#     trial_data_k4.append(deviations_k4)\n",
    "\n",
    "#     # Save averages\n",
    "#     mean_offdiag_k2.append(np.mean(deviations_k2))\n",
    "#     mean_offdiag_k3.append(np.mean(deviations_k3))\n",
    "#     mean_offdiag_k4.append(np.mean(deviations_k4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Plot average L2 norms for κ₂, κ₃, and κ₄\n",
    "# plt.plot(sample_sizes, mean_offdiag_k2, 'o-', label=r'$\\kappa_2$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k3, 's-', label=r'$\\kappa_3$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k4, '^-', label=r'$\\kappa_4$')\n",
    "\n",
    "# # Log scale\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # Labels and title\n",
    "# plt.xlabel('Sample size (N)')\n",
    "# plt.ylabel('Mean off-diagonal L2 norm')\n",
    "# plt.title('Off-diagonal Discrepancy vs. Sample Size')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# output_file = \"cumulant_offdiag_deviation_results_n5.csv\"\n",
    "\n",
    "# with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "    \n",
    "#     # Write header\n",
    "#     header = ['Sample Size', 'Cumulant']\n",
    "#     header += [f'Trial {i+1}' for i in range(n_trials)]\n",
    "#     header += ['Mean']\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # Helper to write one cumulant's data\n",
    "#     def write_cumulant_rows(sample_sizes, trial_data, mean_data, cumulant_label):\n",
    "#         for i, N in enumerate(sample_sizes):\n",
    "#             row = [N, cumulant_label]\n",
    "#             row.extend(trial_data[i])\n",
    "#             row.append(mean_data[i])\n",
    "#             writer.writerow(row)\n",
    "\n",
    "#     # Write data for each cumulant\n",
    "#     write_cumulant_rows(sample_sizes, trial_data_k2, mean_offdiag_k2, 'kappa2')\n",
    "#     write_cumulant_rows(sample_sizes, trial_data_k3, mean_offdiag_k3, 'kappa3')\n",
    "#     write_cumulant_rows(sample_sizes, trial_data_k4, mean_offdiag_k4, 'kappa4')\n",
    "\n",
    "# print(f\"Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off diag L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Per-sample-size trial data\n",
    "# trial_data_k2_L1norm = []\n",
    "# trial_data_k3_L1norm = []\n",
    "# trial_data_k4_L1norm = []\n",
    "\n",
    "# # Per-sample-size mean (for main trend line)\n",
    "# mean_offdiag_k2_L1norm = []\n",
    "# mean_offdiag_k3_L1norm = []\n",
    "# mean_offdiag_k4_L1norm = []\n",
    "\n",
    "# for N in sample_sizes:\n",
    "#     deviations_k2, deviations_k3, deviations_k4 = [], [], []\n",
    "\n",
    "#     for _ in range(n_trials):\n",
    "#         test_samples = gaussian_data_vary_std(N, var)\n",
    "#         cumulants = cumulant_extraction_multidim(test_samples.samples)\n",
    "\n",
    "#         # κ2: remove diagonal\n",
    "#         k2 = cumulants['kappa2'].numpy()\n",
    "#         off_k2 = k2 - np.diag(np.diag(k2))\n",
    "#         deviations_k2.append(np.sum(np.abs(off_k2)))  # L1 norm\n",
    "\n",
    "#         # κ3: mask fully diagonal terms (i=j=k)\n",
    "#         k3 = cumulants['kappa3'].numpy()\n",
    "#         mask3 = np.ones_like(k3, dtype=bool)\n",
    "#         for i in range(k3.shape[0]):\n",
    "#             mask3[i, i, i] = False\n",
    "#         deviations_k3.append(np.sum(np.abs(k3[mask3])))  # L1 norm\n",
    "\n",
    "#         # κ4: mask fully diagonal terms (i=j=k=l)\n",
    "#         k4 = cumulants['kappa4'].numpy()\n",
    "#         mask4 = np.ones_like(k4, dtype=bool)\n",
    "#         for i in range(k4.shape[0]):\n",
    "#             mask4[i, i, i, i] = False\n",
    "#         deviations_k4.append(np.sum(np.abs(k4[mask4])))  # L1 norm\n",
    "\n",
    "#     # Save per-trial data\n",
    "#     trial_data_k2_L1norm.append(deviations_k2)\n",
    "#     trial_data_k3_L1norm.append(deviations_k3)\n",
    "#     trial_data_k4_L1norm.append(deviations_k4)\n",
    "\n",
    "#     # Save averages\n",
    "#     mean_offdiag_k2_L1norm.append(np.mean(deviations_k2))\n",
    "#     mean_offdiag_k3_L1norm.append(np.mean(deviations_k3))\n",
    "#     mean_offdiag_k4_L1norm.append(np.mean(deviations_k4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Plot average L1 norms for κ₂, κ₃, and κ₄\n",
    "# plt.plot(sample_sizes, mean_offdiag_k2_L1norm, 'o-', label=r'$\\kappa_2$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k3_L1norm, 's-', label=r'$\\kappa_3$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k4_L1norm, '^-', label=r'$\\kappa_4$')\n",
    "\n",
    "# # Log scale\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # Labels and title\n",
    "# plt.xlabel('Sample size (N)')\n",
    "# plt.ylabel('Mean off-diagonal L1 norm')  # <-- updated from L2 to L1\n",
    "# plt.title('Off-diagonal Discrepancy vs. Sample Size (L1 Norm)')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define CSV filename\n",
    "# output_file = \"offdiag_cumulant_L1norms_gaussian_task.csv\"\n",
    "\n",
    "# # Open and write to CSV\n",
    "# with open(output_file, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "    \n",
    "#     # Header row\n",
    "#     n_trials = len(trial_data_k2_L1norm[0])\n",
    "#     header = ['SampleSize', 'K2_mean', 'K3_mean', 'K4_mean']\n",
    "#     header += [f'K2_trial_{i+1}' for i in range(n_trials)]\n",
    "#     header += [f'K3_trial_{i+1}' for i in range(n_trials)]\n",
    "#     header += [f'K4_trial_{i+1}' for i in range(n_trials)]\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # Write data per sample size\n",
    "#     for idx, N in enumerate(sample_sizes):\n",
    "#         row = [N,\n",
    "#                mean_offdiag_k2_L1norm[idx],\n",
    "#                mean_offdiag_k3_L1norm[idx],\n",
    "#                mean_offdiag_k4_L1norm[idx]]\n",
    "#         row += trial_data_k2_L1norm[idx]\n",
    "#         row += trial_data_k3_L1norm[idx]\n",
    "#         row += trial_data_k4_L1norm[idx]\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"✅ Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define quadratic activation function\n",
    "alpha = 0.5\n",
    "class QuadraticActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return z + alpha*(z ** 2)\n",
    "\n",
    "# Define model, L=1, N=10, sigma = 0.75\n",
    "class oneDNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \" 1 linear layer, no nonlinearity, no bias \"\n",
    "        # self.non_linear_activ = nn.Sequential(nn.Linear(2,1, bias=False))\n",
    "\n",
    "        \" 1 hidden layer with a quadratic nonlinearity (so can reuse Fischer analysis directly):\"\n",
    "        self.non_linear_activ = nn.Sequential(\n",
    "            nn.Linear(2, 2, bias=False),\n",
    "            # QuadraticActivation(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1, bias=False)\n",
    "\n",
    "            # nn.Linear(50, 50, bias=True),\n",
    "            # QuadraticActivation(),\n",
    "            # nn.Linear(50, 50, bias=True),\n",
    "            # QuadraticActivation(),\n",
    "            # nn.Linear(50, 50, bias=True),\n",
    "            # QuadraticActivation(),\n",
    "        )\n",
    "\n",
    "        \" 2 hidden layers with a quadratic nonlinearity (so can reuse Fischer analysis directly):\"\n",
    "        # self.non_linear_activ = nn.Sequential(\n",
    "        #     nn.Linear(2, 10, bias=False),\n",
    "        #     QuadraticActivation(),\n",
    "        #     nn.Linear(10, 10, bias=False),\n",
    "        #     QuadraticActivation(),\n",
    "        #     nn.Linear(10, 1, bias=False)\n",
    "        # )\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.non_linear_activ(x)\n",
    "        return logits\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for i, m in enumerate(self.non_linear_activ):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if i == 0:\n",
    "                    nn.init.normal_(m.weight, mean=0, std=np.sqrt(0.75/2))\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight, mean=0, std=np.sqrt(0.75/2))\n",
    "                # nn.init.normal_(m.bias, mean=0, std=np.sqrt(0.75))\n",
    "                # nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Model // Cumulant Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_affine_gaussian(mean, covariance, W, b=None):\n",
    "    \"\"\" Adjusted for this task\n",
    "     Propagate the first two cumulants through an affine transformation.\"\"\"\n",
    "    mean = mean.to(torch.float64)\n",
    "    covariance = covariance.to(torch.float64)\n",
    "    W = W.to(torch.float64)\n",
    "\n",
    "#    print(f\"\\n-----------\\nStarting propagation across affine layer\")\n",
    "#     print(f\"Test - printing the shapes of the inputs:\")\n",
    "#     print(f\" Mean shape: {mean.shape}\")\n",
    "#     print(f\" Covariance shape: {covariance.shape}\")\n",
    "#     print(f\" W shape: {W.shape}\") \n",
    "    # if b is not None:\n",
    "    #     print(f\" b shape: {b.shape}\")\n",
    "    \n",
    "    # if mean.ndim == 1:  # Ensure it's a column vector for multiplication\n",
    "    #     mean = mean.unsqueeze(1)  # Shape: [n, 1]\n",
    "\n",
    "\n",
    "    # if mean.shape[0] != W.shape[1]:\n",
    "    #     raise ValueError(f\"Size mismatch: Mean size {mean.shape} is incompatible with weight matrix size {W.shape}.\")\n",
    "\n",
    "    if b is not None:\n",
    "        b = b.to(torch.float64)\n",
    "        if b.ndim == 1:\n",
    "            b = b.unsqueeze(1)\n",
    "        mu_Z = (W @ mean).squeeze() + b.squeeze()\n",
    "    else:\n",
    "        mu_Z = (W @ mean).squeeze()\n",
    "    #     mu_Z = W @ mean.T + b\n",
    "    # else:\n",
    "    #     mu_Z = W @ mean.T\n",
    "    cov_Z = W @ covariance @ W.T\n",
    "\n",
    "    # print(f\"\\n Output mean shape: {mu_Z.shape}\")\n",
    "    # print(f\" Output covariance shape: {cov_Z.shape}\")\n",
    "    # print(f\"Propagated across affine layer complete \\n--------------------------\\n\")\n",
    "\n",
    "    return mu_Z, cov_Z\n",
    "\n",
    "def propagate_affine_higher(g3, g4, W):\n",
    "    \"\"\" Adjusted for this task\n",
    "     Propagate the first two cumulants through an affine transformation.\"\"\"\n",
    "    g3 = g3.to(torch.float64)\n",
    "    g4 = g4.to(torch.float64)\n",
    "    W = W.to(torch.float64)\n",
    "\n",
    "    # print(f\"\\n-----------\\nStarting propagation across affine layer\")\n",
    "    # print(f\"Test - printing the shapes of the inputs:\")\n",
    "    # print(f\" Mean shape: {g3.shape}\")\n",
    "    # print(f\" Covariance shape: {g4.shape}\")\n",
    "    # print(f\" W shape: {W.shape}\")\n",
    "    \n",
    "    # Propagate 3rd order cumulant\n",
    "    g3_z = torch.einsum('ri,sj,tk,ijk->rst', W, W, W, g3)\n",
    "\n",
    "    # Propagate 4th order cumulant\n",
    "    g4_z = torch.einsum('ri,sj,tk,ul,ijkl->rstu', W, W, W, W, g4)\n",
    "\n",
    "    # print(f\"Output cumulant shapes:\")\n",
    "    # print(f\"  g3_z shape: {g3_z.shape}\")\n",
    "    # print(f\"  g4_z shape: {g4_z.shape}\")\n",
    "    # print(f\"Propagation across affine layer complete\\n--------------------------\\n\")\n",
    "\n",
    "    return g3_z, g4_z\n",
    "\n",
    "def propagate_quadratic_gaussian(mean, covariance):\n",
    "    \"\"\" Propagate cumulants through a quadratic activation function\n",
    "    Assume the Fischer Gaussian approx, ie this is for wide hidden layers - so only first two cumulants get propagated \"\"\"\n",
    "\n",
    "    # # assuming cumulants higher than second order vanish - which they do for this input distribution\n",
    "    # diagonal_cov = torch.diagonal(covariance, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "    # mu_y = mean + alpha * (mean ** 2 + diagonal_cov)\n",
    "\n",
    "    # cov_y = covariance + 2 * alpha * covariance * (mean.unsqueeze(-1) + mean.unsqueeze(-2)) \\\n",
    "    #     + 2 * alpha**2 * (covariance @ covariance) \\\n",
    "    #     + 4 * alpha**2 * mean.unsqueeze(-1) * covariance * mean.unsqueeze(-2)\n",
    "\n",
    "    # return mu_y, cov_y    \n",
    "\n",
    "    diagonal_cov = torch.diagonal(covariance, dim1=0, dim2=1)  # Shape: [n]\n",
    "    \n",
    "    # Mean propagation (remains a 1D vector of size [n])\n",
    "    mu_y = mean + alpha * (mean ** 2 + diagonal_cov)\n",
    "    \n",
    "    # Covariance propagation (remains a 2D matrix of size [n, n])\n",
    "    mean_outer = mean.unsqueeze(1) * mean.unsqueeze(0)  # Shape: [n, n]\n",
    "\n",
    "    cov_y = covariance + 2 * alpha * (covariance * (mean.unsqueeze(0) + mean.unsqueeze(1))) \\\n",
    "            + 2 * alpha**2 * (covariance @ covariance) \\\n",
    "            + 4 * alpha**2 * mean_outer * covariance\n",
    "\n",
    "    return mu_y, cov_y\n",
    "\n",
    "def propagate_full_gaussian_only(cumulants, weights, biases=None):\n",
    "    \"\"\" Propagates input cumulants through the network (that uses quadratic activation)\n",
    "    Capable of handling varying numbers of layers (we just extract the weights and biases from the model).\n",
    "    \n",
    "     This function only propagates the first two cumulants - ie we truncate, effectively assume all higher ones = 0 \"\"\"\n",
    "\n",
    "    # mu_list = []\n",
    "    # cov_list = []\n",
    "    # for i in range(len(mean_values)):\n",
    "    #     mu = mean_values[i].to(torch.float64)\n",
    "    #     cov = covariances[i].to(torch.float64)\n",
    "\n",
    "    # Extract cumulants\n",
    "    mu = cumulants['kappa1']\n",
    "    cov = cumulants['kappa2']\n",
    "    # G3 = cumulants['kappa3']\n",
    "    # G4 = cumulants['kappa4']\n",
    "\n",
    "\n",
    "    for layer_idx in range(len(weights)):\n",
    "        W = weights[layer_idx].to(torch.float64)\n",
    "        if biases is not None:\n",
    "            bi =  biases[layer_idx].to(torch.float64)\n",
    "\n",
    "        mu = mu.reshape(-1) # Flatten mu to a row vector - this is a quirk of pytorch matrix multiplication\n",
    "        if biases is not None:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W, bi)\n",
    "            # print(f\" Propagated across affine, WITH BIASES!: \\n mu: {mu} \\n cov: {cov}\")\n",
    "        else:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W)\n",
    "            # print(f\"Propagated across affine: \\n mu: {mu} \\n cov: {cov}\")\n",
    "        if layer_idx < len(weights) - 1:\n",
    "            mu, cov = propagate_quadratic_gaussian(mu, cov)\n",
    "            # print(f\"Propagated across quadratic: \\n mu: {mu} \\n cov: {cov}\")\n",
    "    \n",
    "    return mu, cov\n",
    "\n",
    "def propagate_full_linear(cumulants, weights, biases=None):\n",
    "    \"\"\"Propagates input cumulants through a purely linear network.\n",
    "    No nonlinear activation functions are used.\n",
    "    Gaussian approximation is irrelevant here as we aren't using nonlinearities.\"\"\"\n",
    "    \n",
    "    mu, cov, g3, g4 = cumulants['kappa1'], cumulants['kappa2'], cumulants['kappa3'], cumulants['kappa4']\n",
    "\n",
    "    for layer_idx in range(len(weights)):\n",
    "        W = weights[layer_idx].to(torch.float64)\n",
    "        if biases is not None:\n",
    "            b =  biases[layer_idx].to(torch.float64)\n",
    "\n",
    "        mu = mu.reshape(-1) # Flatten mu to a row vector - this is a quirk of pytorch matrix multiplication\n",
    "        if biases is not None:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W, b)\n",
    "            # print(f\" Propagated across affine: \\n mu: {mu} \\n cov: {cov}\")\n",
    "            g3, g4 = propagate_affine_higher(g3, g4, W)\n",
    "            # print(f\" Propagated across affine: \\n g3: {g3} \\n g4: {g4}\")\n",
    "        else:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W)\n",
    "            # print(f\"Propagated across affine: \\n mu: {mu} \\n cov: {cov}\")\n",
    "            g3, g4 = propagate_affine_higher(g3, g4, W)\n",
    "            # print(f\" Propagated across affine: \\n g3: {g3} \\n g4: {g4}\")\n",
    "\n",
    "    return mu, cov, g3, g4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher Orders\n",
    "The below code was an attempt at using higher order cumulants in the input layer, couldn't get it to work though.\n",
    "\n",
    "Separated out the code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_affine_with_higher(mean, covariance, G3, G4, W, b=None):\n",
    "    \"\"\" Adjusted for this task\n",
    "     Propagate the first four cumulants through an affine transformation.\"\"\"\n",
    "    mean = mean.to(torch.float64)\n",
    "    covariance = covariance.to(torch.float64)\n",
    "    W = W.to(torch.float64)\n",
    "    if b is not None:\n",
    "        b = b.to(torch.float64)\n",
    "        mu_Z = W @ mean.T + b.unsqueeze(1)\n",
    "\n",
    "    else:\n",
    "        mu_Z = W @ mean.T\n",
    "    cov_Z = W @ covariance @ W.T\n",
    "\n",
    "    G3_z = np.einsum('ia,jb,kc,abc->ijk', W, W, W, G3)\n",
    "    G4_z = np.einsum('ia,jb,kc,ld,abcd->ijkl', W, W, W, W, G4)\n",
    "\n",
    "    G3_z = torch.tensor(G3_z, dtype=torch.float64)\n",
    "    G4_z = torch.tensor(G4_z, dtype=torch.float64)\n",
    "\n",
    "\n",
    "    return mu_Z.T, cov_Z, G3_z, G4_z\n",
    "\n",
    "def propagate_quadratic_with_higher(mean, covariance, G3, G4):\n",
    "    \"\"\" Propagate cumulants through a quadratic activation function\n",
    "     This is for the input layer - so need to consider how all higher order cumulants \n",
    "        contribute to the mean/cov of postactivations, not just propagating the first two only across the activation \n",
    "        \n",
    "    Mean should be a vector of shape (n,) otherwise the higher order contribution to the cov will throw errors\"\"\"\n",
    "    \n",
    "    # mean\n",
    "    diagonal_cov = torch.diagonal(covariance, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "    mu_y = mean + alpha * (mean ** 2 + diagonal_cov)\n",
    "\n",
    "    # cov\n",
    "    cov_y = covariance + 2 * alpha * covariance * (mean.unsqueeze(-1) + mean.unsqueeze(-2)) \\\n",
    "        + 2 * alpha**2 * (covariance @ covariance) \\\n",
    "        + 4 * alpha**2 * mean.unsqueeze(-1) * covariance * mean.unsqueeze(-2)\n",
    "\n",
    "    # Find the higher order cumulant contribution\n",
    "    n = mean.shape[0] \n",
    "\n",
    "    term1 = alpha * (1 + 2 * alpha * mean.unsqueeze(-1)) * G3  # Shape: (n, n, n)\n",
    "    term1 = term1.sum(dim=2)  # Sum over the last axis to get a matrix of shape (n, n)\n",
    "\n",
    "    term2 = alpha * (1 + 2 * alpha * mean.unsqueeze(-2)) * G3  # Shape: (n, n, n)\n",
    "    term2 = term2.sum(dim=0)  # Sum over the first axis to get a matrix of shape (n, n)\n",
    "\n",
    "    term3 = alpha**2 * G4.sum(dim=(0, 2))  # Sum over axes 1, 3 (ie picking the first i,j to sum over) for a matrix of shape (n, n)\n",
    "\n",
    "    # Combine all terms\n",
    "    higher_order_term = term1 + term2 + term3\n",
    "    cov_y += higher_order_term\n",
    "\n",
    "\n",
    "    return mu_y, cov_y\n",
    "\n",
    "def propagate_full_with_higher(cumulants, weights, biases=None):\n",
    "    \"\"\" Propagates input cumulants through the network (that uses quadratic activation)\n",
    "    Capable of handling varying numbers of layers (we just extract the weights and biases from the model).\n",
    "    \n",
    "     This function considers higher cumulants when propagating across input layer, but \n",
    "        truncates for rest of network (ie assumes higher cumulants vanish) \"\"\"\n",
    "\n",
    "    # Extract cumulants\n",
    "    mu = cumulants['kappa1']\n",
    "    cov = cumulants['kappa2']\n",
    "    G3 = cumulants['kappa3']\n",
    "    G4 = cumulants['kappa4']\n",
    "\n",
    "    # Initial affine layer - propagate all cumulants\n",
    "    W = weights[0].to(torch.float64)\n",
    "    if biases is not None:\n",
    "        b = biases[0].to(torch.float64)\n",
    "    mu = mu.view(-1)  # Flatten mu to a row vector (PyTorch quirk)\n",
    "    \n",
    "    if biases is not None:\n",
    "        mu, cov, G3, G4 = propagate_affine_with_higher(mu, cov, G3, G4, W, b)\n",
    "    else:\n",
    "        mu, cov, G3, G4 = propagate_affine_with_higher(mu, cov, G3, G4, W)\n",
    "\n",
    "    # First nonlinearity - propagate all cumulants with propagate_quadratic_with_higher()\n",
    "    mu, cov = propagate_quadratic_with_higher(mu, cov, G3, G4)\n",
    "\n",
    "    \n",
    "    for layer_idx in range(1, len(weights)): # start from second layer\n",
    "        W = weights[layer_idx].to(torch.float64)\n",
    "        if biases is not None:\n",
    "            b = biases[layer_idx].to(torch.float64)\n",
    "\n",
    "        # Ensure mu is properly shaped before matrix multiplication\n",
    "        if mu.dim() == 1:  # If `mu` is a 1D vector\n",
    "            mu = mu.view(-1)  # Ensure it is flat, (in_features,)\n",
    "        elif mu.dim() == 2 and mu.size(0) == 1:  # If it's a row vector, reshape it\n",
    "            mu = mu.squeeze(0)\n",
    "\n",
    "        if biases is not None:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W, b)\n",
    "        else:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W)\n",
    "        if layer_idx < len(weights) - 1:\n",
    "            mu, cov = propagate_quadratic_gaussian(mu, cov)\n",
    "\n",
    "    \n",
    "    return mu, cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to enforce symmetric weights constraints\n",
    "@torch.no_grad()\n",
    "def enforce_weight_constraints(model):\n",
    "    \"\"\"\n",
    "    Enforce symmetry in the first layer (2x2) and equal weights in the second layer (1x2).\n",
    "    \"\"\"\n",
    "    layers = model.non_linear_activ\n",
    "    layer1 = layers[0]\n",
    "    layer2 = layers[2]\n",
    "\n",
    "    # First layer: symmetric 2x2 weights\n",
    "    if isinstance(layer1, nn.Linear) and layer1.weight.shape == (2, 2):\n",
    "        W = layer1.weight.data\n",
    "\n",
    "        # Extract current entries\n",
    "        a = 0.5 * (W[0, 0] + W[1, 1])  # average of diagonals\n",
    "        b = 0.5 * (W[0, 1] + W[1, 0])  # average of off-diagonals\n",
    "\n",
    "        # Construct constrained matrix\n",
    "        constrained_W = torch.tensor([[a, b],\n",
    "                                      [b, a]], dtype=W.dtype, device=W.device)\n",
    "\n",
    "        layer1.weight.data.copy_(constrained_W)\n",
    "\n",
    "        # Enforce bias vector [b, b]\n",
    "        if layer1.bias is not None:\n",
    "            b = layer1.bias.data\n",
    "            avg_bias = b.mean()\n",
    "            b.fill_(avg_bias)\n",
    "            layer1.bias.data.copy_(b)\n",
    "    \n",
    "    # Second layer: equal weights (1x2)\n",
    "    # Need to change this from 1 to 2 if including nonlinearity\n",
    "    if isinstance(layer2, nn.Linear) and layer2.weight.shape == (1, 2):\n",
    "        W = layer2.weight.data\n",
    "        avg_value = W.mean()\n",
    "        W.fill_(avg_value)\n",
    "        layer2.weight.data.copy_(W)\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_dead_neurons(model, dataloader, threshold=0.99):\n",
    "    \"\"\"\n",
    "    Check for dead neurons in ReLU layers by computing how often their output is zero.\n",
    "\n",
    "    Args:\n",
    "        model: the trained model\n",
    "        dataloader: data loader with input samples\n",
    "        threshold: fraction of zeros above which a neuron is considered dead\n",
    "\n",
    "    Returns:\n",
    "        Dict of dead neuron indices and percentages per layer\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dead_report = {}\n",
    "    \n",
    "    for i, layer in enumerate(model.non_linear_activ):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            continue  # skip linear layers directly\n",
    "        if isinstance(layer, nn.ReLU) or isinstance(layer, QuadraticActivation):\n",
    "            # Only apply this for real ReLU-like activations, or Quadratic if you ever extend it\n",
    "            layer_outputs = []\n",
    "\n",
    "            for X, _ in dataloader:\n",
    "                X = X.to(device)\n",
    "                with torch.no_grad():\n",
    "                    # Forward until this layer\n",
    "                    out = X\n",
    "                    for j in range(i + 1):\n",
    "                        out = model.non_linear_activ[j](out)\n",
    "                    layer_outputs.append(out.cpu())\n",
    "\n",
    "            layer_outputs = torch.cat(layer_outputs, dim=0)\n",
    "            zero_mask = (layer_outputs == 0)\n",
    "            zero_fraction = zero_mask.sum(dim=0).float() / zero_mask.shape[0]\n",
    "\n",
    "            dead_neurons = (zero_fraction > threshold).nonzero(as_tuple=True)[0].tolist()\n",
    "            dead_report[f\"Layer {i}\"] = {\n",
    "                \"dead_neurons\": dead_neurons,\n",
    "                \"fractions\": zero_fraction.numpy()\n",
    "            }\n",
    "\n",
    "    return dead_report\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, track_loss=False, track_params=False, symmetry=False):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    loss_steps = []\n",
    "    loss_values = []\n",
    "    param_steps = []\n",
    "    param_changes = []\n",
    "    step = 0\n",
    "\n",
    "    if track_params:\n",
    "        prev_params = {name: param.clone().detach() for  name, param in model.named_parameters()}\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device).unsqueeze(1)\n",
    "\n",
    "        # Compute prediction error for forward pass on current batch\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if symmetry:\n",
    "            # 🔧 Enforce symmetry and equal weights\n",
    "            enforce_weight_constraints(model)\n",
    "\n",
    "        \n",
    "        # Track loss\n",
    "        if track_loss:\n",
    "            loss_steps.append(step)\n",
    "            loss_values.append(loss.item()) # this is already a per-batch average loss\n",
    "        \n",
    "        # Track parameter changes every 10 steps/batches\n",
    "        if track_params and step % 10 ==0 and step > 0:\n",
    "            total_param_change = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                param_diff = param.detach() - prev_params[name]\n",
    "                # total_param_change += torch.norm(param_diff, p=2).item()\n",
    "                total_param_change += torch.sum(param_diff ** 2).item()  # Direct sum of squared differences\n",
    "                prev_params[name] = param.clone().detach()\n",
    "            param_steps.append(step)\n",
    "            param_changes.append(np.sqrt(total_param_change))\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Current batch: {current:>5d}/{size:>5d}\\n  Loss of batch: {loss:>7f}\")\n",
    "\n",
    "        step += 1\n",
    "    \n",
    "    return loss_steps, loss_values, param_steps, param_changes\n",
    "\n",
    "\n",
    "def collect_model_outputs(dataloader, model):\n",
    "    \"\"\"\n",
    "    Performs a single forwards pass, then collects all outputs from the model for the given dataloader and\n",
    "      stacks them into a 1D numpy array.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (DataLoader): The DataLoader to iterate over.\n",
    "        model (torch.nn.Module): The trained model whose outputs are to be collected.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D NumPy array of all model outputs concatenated together.\n",
    "    \"\"\"\n",
    "    model.eval()  # Switch to evaluation mode (no gradients tracked)\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for X, _ in dataloader:\n",
    "            X = X.to(device)\n",
    "            outputs = model(X)\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())  # Ensure outputs are 1D and on CPU\n",
    "\n",
    "    # Concatenate all outputs into a single 1D NumPy array of size (N,)\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "\n",
    "def plot_histogram(data, title=None, bins=50, color='blue', show=False, norm=False, label=None, legend=None, edgecolor='black'):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data to plot.\n",
    "        title (str): Title of the plot.\n",
    "        bins (int): Number of bins in the histogram.\n",
    "        color (str): Color of the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    if norm:\n",
    "        plt.hist(data, bins=bins, color=color, label=label, density=True, alpha=0.7, edgecolor=edgecolor)\n",
    "    else:\n",
    "        plt.hist(data, bins=bins, color=color, alpha=0.7, edgecolor='black')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network Model\n",
    "\n",
    "Using MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch:    64/1000000\n",
      "  Loss of batch: 0.744760\n",
      "Current batch: 32064/1000000\n",
      "  Loss of batch: 0.679595\n",
      "Current batch: 64064/1000000\n",
      "  Loss of batch: 0.384088\n",
      "Current batch: 96064/1000000\n",
      "  Loss of batch: 0.299911\n",
      "Current batch: 128064/1000000\n",
      "  Loss of batch: 0.558627\n",
      "Current batch: 160064/1000000\n",
      "  Loss of batch: 0.635131\n",
      "Current batch: 192064/1000000\n",
      "  Loss of batch: 0.471484\n",
      "Current batch: 224064/1000000\n",
      "  Loss of batch: 0.547440\n",
      "Current batch: 256064/1000000\n",
      "  Loss of batch: 0.562075\n",
      "Current batch: 288064/1000000\n",
      "  Loss of batch: 0.469439\n",
      "Current batch: 320064/1000000\n",
      "  Loss of batch: 0.735276\n",
      "Current batch: 352064/1000000\n",
      "  Loss of batch: 0.415273\n",
      "Current batch: 384064/1000000\n",
      "  Loss of batch: 0.340521\n",
      "Current batch: 416064/1000000\n",
      "  Loss of batch: 0.445540\n",
      "Current batch: 448064/1000000\n",
      "  Loss of batch: 0.458077\n",
      "Current batch: 480064/1000000\n",
      "  Loss of batch: 0.266255\n",
      "Current batch: 512064/1000000\n",
      "  Loss of batch: 0.399216\n",
      "Current batch: 544064/1000000\n",
      "  Loss of batch: 0.619443\n",
      "Current batch: 576064/1000000\n",
      "  Loss of batch: 0.574784\n",
      "Current batch: 608064/1000000\n",
      "  Loss of batch: 0.379062\n",
      "Current batch: 640064/1000000\n",
      "  Loss of batch: 0.296305\n",
      "Current batch: 672064/1000000\n",
      "  Loss of batch: 0.772367\n",
      "Current batch: 704064/1000000\n",
      "  Loss of batch: 0.404506\n",
      "Current batch: 736064/1000000\n",
      "  Loss of batch: 0.438493\n",
      "Current batch: 768064/1000000\n",
      "  Loss of batch: 0.402883\n",
      "Current batch: 800064/1000000\n",
      "  Loss of batch: 0.513121\n",
      "Current batch: 832064/1000000\n",
      "  Loss of batch: 0.693938\n",
      "Current batch: 864064/1000000\n",
      "  Loss of batch: 0.477264\n",
      "Current batch: 896064/1000000\n",
      "  Loss of batch: 0.968222\n",
      "Current batch: 928064/1000000\n",
      "  Loss of batch: 0.394851\n",
      "Current batch: 960064/1000000\n",
      "  Loss of batch: 0.550025\n",
      "Current batch: 992064/1000000\n",
      "  Loss of batch: 0.683313\n",
      "Current batch:    64/1000000\n",
      "  Loss of batch: 0.673161\n",
      "Current batch: 32064/1000000\n",
      "  Loss of batch: 0.387964\n",
      "Current batch: 64064/1000000\n",
      "  Loss of batch: 0.449217\n",
      "Current batch: 96064/1000000\n",
      "  Loss of batch: 0.595660\n",
      "Current batch: 128064/1000000\n",
      "  Loss of batch: 0.688611\n",
      "Current batch: 160064/1000000\n",
      "  Loss of batch: 0.419434\n",
      "Current batch: 192064/1000000\n",
      "  Loss of batch: 0.751802\n",
      "Current batch: 224064/1000000\n",
      "  Loss of batch: 0.496382\n",
      "Current batch: 256064/1000000\n",
      "  Loss of batch: 0.495011\n",
      "Current batch: 288064/1000000\n",
      "  Loss of batch: 0.520165\n",
      "Current batch: 320064/1000000\n",
      "  Loss of batch: 0.603299\n",
      "Current batch: 352064/1000000\n",
      "  Loss of batch: 0.413855\n",
      "Current batch: 384064/1000000\n",
      "  Loss of batch: 0.517067\n",
      "Current batch: 416064/1000000\n",
      "  Loss of batch: 0.579915\n",
      "Current batch: 448064/1000000\n",
      "  Loss of batch: 0.614929\n",
      "Current batch: 480064/1000000\n",
      "  Loss of batch: 0.519149\n",
      "Current batch: 512064/1000000\n",
      "  Loss of batch: 0.290321\n",
      "Current batch: 544064/1000000\n",
      "  Loss of batch: 0.311891\n",
      "Current batch: 576064/1000000\n",
      "  Loss of batch: 0.480859\n",
      "Current batch: 608064/1000000\n",
      "  Loss of batch: 0.238135\n",
      "Current batch: 640064/1000000\n",
      "  Loss of batch: 0.398398\n",
      "Current batch: 672064/1000000\n",
      "  Loss of batch: 0.345920\n",
      "Current batch: 704064/1000000\n",
      "  Loss of batch: 0.710136\n",
      "Current batch: 736064/1000000\n",
      "  Loss of batch: 0.351946\n",
      "Current batch: 768064/1000000\n",
      "  Loss of batch: 0.728112\n",
      "Current batch: 800064/1000000\n",
      "  Loss of batch: 0.442063\n",
      "Current batch: 832064/1000000\n",
      "  Loss of batch: 0.459888\n",
      "Current batch: 864064/1000000\n",
      "  Loss of batch: 0.266362\n",
      "Current batch: 896064/1000000\n",
      "  Loss of batch: 0.496397\n",
      "Current batch: 928064/1000000\n",
      "  Loss of batch: 0.399037\n",
      "Current batch: 960064/1000000\n",
      "  Loss of batch: 0.728597\n",
      "Current batch: 992064/1000000\n",
      "  Loss of batch: 0.637688\n",
      "Current batch:    64/1000000\n",
      "  Loss of batch: 0.501454\n",
      "Current batch: 32064/1000000\n",
      "  Loss of batch: 0.428003\n",
      "Current batch: 64064/1000000\n",
      "  Loss of batch: 0.655319\n",
      "Current batch: 96064/1000000\n",
      "  Loss of batch: 0.505064\n",
      "Current batch: 128064/1000000\n",
      "  Loss of batch: 0.701021\n",
      "Current batch: 160064/1000000\n",
      "  Loss of batch: 0.428511\n",
      "Current batch: 192064/1000000\n",
      "  Loss of batch: 0.414903\n",
      "Current batch: 224064/1000000\n",
      "  Loss of batch: 0.307477\n",
      "Current batch: 256064/1000000\n",
      "  Loss of batch: 0.423327\n",
      "Current batch: 288064/1000000\n",
      "  Loss of batch: 0.487168\n",
      "Current batch: 320064/1000000\n",
      "  Loss of batch: 0.468902\n",
      "Current batch: 352064/1000000\n",
      "  Loss of batch: 0.663931\n",
      "Current batch: 384064/1000000\n",
      "  Loss of batch: 0.530652\n",
      "Current batch: 416064/1000000\n",
      "  Loss of batch: 0.387900\n",
      "Current batch: 448064/1000000\n",
      "  Loss of batch: 0.489980\n",
      "Current batch: 480064/1000000\n",
      "  Loss of batch: 0.309564\n",
      "Current batch: 512064/1000000\n",
      "  Loss of batch: 0.428873\n",
      "Current batch: 544064/1000000\n",
      "  Loss of batch: 0.313117\n",
      "Current batch: 576064/1000000\n",
      "  Loss of batch: 0.606971\n",
      "Current batch: 608064/1000000\n",
      "  Loss of batch: 0.321460\n",
      "Current batch: 640064/1000000\n",
      "  Loss of batch: 0.357553\n",
      "Current batch: 672064/1000000\n",
      "  Loss of batch: 0.474091\n",
      "Current batch: 704064/1000000\n",
      "  Loss of batch: 0.523151\n",
      "Current batch: 736064/1000000\n",
      "  Loss of batch: 0.404080\n",
      "Current batch: 768064/1000000\n",
      "  Loss of batch: 0.438559\n",
      "Current batch: 800064/1000000\n",
      "  Loss of batch: 0.542596\n",
      "Current batch: 832064/1000000\n",
      "  Loss of batch: 0.686316\n",
      "Current batch: 864064/1000000\n",
      "  Loss of batch: 0.469599\n",
      "Current batch: 896064/1000000\n",
      "  Loss of batch: 0.481633\n",
      "Current batch: 928064/1000000\n",
      "  Loss of batch: 0.377944\n",
      "Current batch: 960064/1000000\n",
      "  Loss of batch: 0.279748\n",
      "Current batch: 992064/1000000\n",
      "  Loss of batch: 0.695515\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "\n",
      "Name: non_linear_activ.0.weight\n",
      "Shape: torch.Size([2, 2])\n",
      "Values: tensor([[0.6517, 0.6517],\n",
      "        [0.6517, 0.6517]])\n",
      "Name: non_linear_activ.2.weight\n",
      "Shape: torch.Size([1, 2])\n",
      "Values: tensor([[0.5425, 0.5425]])\n"
     ]
    }
   ],
   "source": [
    "model = oneDNetwork().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "\n",
    "# train, plot loss\n",
    "tot_loss = []\n",
    "tot_steps_loss = []\n",
    "# tot_param_changes = []\n",
    "# tot_param_steps = []\n",
    "loss_step_count = 0\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss1_steps, loss1_values, param1_steps, param1_changes = train(trainloader, model, loss_fn, optimizer, track_loss=True, track_params=False, symmetry=True)\n",
    "    # param1_steps = [x + loss_step_count for x in param1_steps]\n",
    "    loss1_steps = [x + loss_step_count for x in loss1_steps]\n",
    "    loss_step_count += len(loss1_steps)\n",
    "\n",
    "    tot_loss.extend(loss1_values)\n",
    "    tot_steps_loss.extend(loss1_steps)\n",
    "    # tot_param_changes.extend(param1_changes)\n",
    "    # tot_param_steps.extend(param1_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check for dead neurons\n",
    "dead_info = check_dead_neurons(model, trainloader)\n",
    "\n",
    "# Extract per-neuron zero output fraction (for the single nonlinear layer)\n",
    "# Assuming only one nonlinear layer, so we just grab its first entry\n",
    "dead_layer_key = list(dead_info.keys())[0]  # e.g., 'Layer 1'\n",
    "zero_fractions = dead_info[dead_layer_key]['fractions']  # This is a NumPy array\n",
    "\n",
    "\n",
    "# Optionally also write the total dead neuron *fraction* (e.g., number dead / total)\n",
    "n_neurons = len(zero_fractions)\n",
    "n_dead = len(dead_info[dead_layer_key][\"dead_neurons\"])\n",
    "total_dead_frac = n_dead / n_neurons if n_neurons > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(tot_steps_loss, tot_loss, label='Loss', color='blue', alpha=0.7)\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Network Loss $\\mathcal{L}$\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print out model parameters to check\n",
    "print(\"Model Parameters:\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param.data}\")\n",
    "\n",
    "# Save the model weights to a file (e.g., 'model_weights.pth')\n",
    "model_weights = {name: param.data for name, param in model.named_parameters()}\n",
    "torch.save(model_weights, 'weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "These are asymmetric solutions - not consistent with my analytical results.\n",
    "\n",
    "Only sometimes does it seem to converge to the expected 'columns same'/'across diagonals' symmetric cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Histogram + Overlay\n",
    "\n",
    "Here we want to compare a histogram of the raw network outputs to an overlaid plot of the propagated theoretical probability density (using Fischer framework).\n",
    "\n",
    "This functions as a check of the Fischer framework validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output cumulants\n",
    "def find_network_output_cumulants(data_loader, model_x):\n",
    "\n",
    "    network_outputs = collect_model_outputs(data_loader, model_x)\n",
    "    network_cumulants = cumulant_extraction(network_outputs)\n",
    "    print(\"\\nExtracted Cumulants (of raw network output):\")\n",
    "    for k, v in network_cumulants.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return network_outputs, network_cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pdf_from_cumulants(y_vals, cumulants, t_max=50.0, num_points=2**12):\n",
    "    \"\"\"\n",
    "    Construct a PDF from a list of cumulants using characteristic function inversion.\n",
    "\n",
    "    Args:\n",
    "        y_vals (np.ndarray): Points at which to evaluate the PDF.\n",
    "        cumulants (list): List of raw cumulants (may include PyTorch tensors).\n",
    "        t_max (float): Max frequency for integration.\n",
    "        num_points (int): Number of FFT points (preferably power of 2).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Approximated PDF values.\n",
    "    \"\"\"\n",
    "    t = np.linspace(-t_max, t_max, num_points)\n",
    "    dt = t[1] - t[0]\n",
    "\n",
    "    log_phi = np.zeros_like(t, dtype=np.complex128)\n",
    "\n",
    "    for n, kappa in enumerate(cumulants, start=1):\n",
    "        if hasattr(kappa, 'item'):  # Convert PyTorch tensor to scalar\n",
    "            kappa = kappa.item()\n",
    "        log_phi += (1j * t)**n * kappa / math.factorial(n)\n",
    "\n",
    "    log_phi = np.clip(np.real(log_phi), -500, 500) + 1j * np.imag(log_phi)\n",
    "    phi = np.exp(log_phi)\n",
    "\n",
    "    # Inverse Fourier transform\n",
    "    y_grid = np.linspace(-np.pi / dt, np.pi / dt, num_points)\n",
    "    fft_pdf = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(phi))) * dt / (2 * np.pi)\n",
    "\n",
    "    # Interpolate onto desired y_vals\n",
    "    interp_pdf = np.interp(y_vals, y_grid, np.real(fft_pdf))\n",
    "\n",
    "    # Normalize\n",
    "    interp_pdf /= np.trapezoid(interp_pdf, y_vals)\n",
    "\n",
    "    return interp_pdf\n",
    "\n",
    "def theoretical_pdf_normal(y_vals, mu, cov):\n",
    "    \"\"\"  \n",
    "    Plot Theoretical Probability Density Function using a single 1D mean and variance.\n",
    "    \n",
    "    Args:\n",
    "    - y_vals (np.ndarray): Array of y values where the PDF is evaluated.\n",
    "    - mu (torch.Tensor): Scalar tensor representing the mean.\n",
    "    - cov (torch.Tensor): Scalar tensor representing the variance.\n",
    "    \n",
    "    Returns:\n",
    "    - pdf (np.ndarray): Theoretical probability density function values.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy scalars\n",
    "    mu = mu.item()\n",
    "    variance = cov.item()\n",
    "    sigma = variance ** 0.5  # Convert variance to standard deviation\n",
    "\n",
    "    # Compute the PDF using the normal distribution\n",
    "    pdf = norm.pdf(y_vals, loc=mu, scale=sigma)\n",
    "\n",
    "    # Normalize the PDF so it integrates to 1\n",
    "    pdf /= np.trapezoid(pdf, y_vals)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "# def theoretical_pdf_higher(y_vals, k1, k2, k3, k4):\n",
    "    \"\"\"  \n",
    "    Plot Theoretical PDF using the first four cumulants via Edgeworth expansion.\n",
    "    \n",
    "    Args:\n",
    "    - y_vals (np.ndarray): Points at which to evaluate the PDF.\n",
    "    - k1 (torch.Tensor): First cumulant (mean).\n",
    "    - k2 (torch.Tensor): Second cumulant (variance).\n",
    "    - k3 (torch.Tensor): Third cumulant.\n",
    "    - k4 (torch.Tensor): Fourth cumulant.\n",
    "    \n",
    "    Returns:\n",
    "    - pdf (np.ndarray): Approximated PDF values.\n",
    "    \"\"\"\n",
    "    # Convert to scalars\n",
    "    mu = k1.item()\n",
    "    var = k2.item()\n",
    "    c3 = k3.item()\n",
    "    c4 = k4.item()\n",
    "    \n",
    "    sigma = np.sqrt(var)\n",
    "    z = (y_vals - mu) / sigma\n",
    "    \n",
    "    # Standard normal PDF\n",
    "    phi = norm.pdf(z)\n",
    "    \n",
    "    # Hermite polynomials\n",
    "    H3 = hermite(3)(z)\n",
    "    H4 = hermite(4)(z)\n",
    "\n",
    "    # Edgeworth correction using raw cumulants\n",
    "    skew_term = (c3 / (6 * sigma**3)) * H3\n",
    "    kurt_term = (c4 / (24 * sigma**4)) * H4\n",
    "    edgeworth_corr = 1 + skew_term + kurt_term\n",
    "\n",
    "    pdf = phi * edgeworth_corr / sigma  # De-standardize\n",
    "\n",
    "    # Normalize to integrate to 1\n",
    "    pdf /= np.trapezoid(pdf, y_vals)\n",
    "\n",
    "    pdf = np.clip(pdf, 0, None)\n",
    "\n",
    "    return pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate Cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 0\n",
      "weights: [tensor([[0.6517, 0.6517],\n",
      "        [0.6517, 0.6517]]), tensor([[0.5425, 0.5425]])]\n",
      "biases: []\n",
      "\n",
      "Expected cumulants for uniform distribution: \n",
      " 0.5, 0.08333333333333333, 0, -0.008333333333333333\n",
      "\n",
      "Extracted Cumulants (of raw network output):\n",
      "kappa1: 1.1282053013239812\n",
      "kappa2: 2.726693076786895\n",
      "kappa3: 7.386760307103784\n",
      "kappa4: 17.86291958323415\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.special import hermite\n",
    "\n",
    "# # If not including training then need to initialise the model here:\n",
    "# model = oneDNetwork().to(device)\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# First extract weights and biases from the model\n",
    "def extract_weights(model):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer in model.non_linear_activ:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            W = layer.weight.detach().cpu().numpy()\n",
    "            weights.append(torch.tensor(W, dtype=torch.float64))\n",
    "            if layer.bias is not None:\n",
    "                b = layer.bias.detach().cpu().numpy()\n",
    "                biases.append(torch.tensor(b, dtype=torch.float64))\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = extract_weights(model)\n",
    "print(f\"{len(weights)}, {len(biases)}\")\n",
    "print(f\"weights: {weights}\")\n",
    "print(f\"biases: {biases}\")\n",
    "\n",
    "# Find cumulants\n",
    "print(f\"\\nExpected cumulants for uniform distribution: \\n {1/2}, {1/12}, {0}, {-1/120}\")  # Expected cumulants for uniform distribution\n",
    "avg_input_c, z_dict = find_input_cumulants(test_samples)\n",
    "avg_jl_cumulants, ground_truth_c = find_pred_output_cumulants(test_samples)\n",
    "network_outputs, network_cumulants = find_network_output_cumulants(testloader, model)\n",
    "\n",
    "\n",
    "# # Propagate cumulants\n",
    "\n",
    "# # Networks with nonlinearity\n",
    "# mu_out, cov_out = propagate_full_gaussian_only(z_dict, weights, biases=None) # if using biases, set biases = True\n",
    "# cumulants_plotting = [mu_out, cov_out]\n",
    "\n",
    "# # # Networks without nonlinearity\n",
    "# # mu_out, cov_out, g3_out, g4_out = propagate_full_linear(z_dict, weights, biases=None) # if using biases, set biases = True\n",
    "# # cumulants_plotting = [mu_out, cov_out, g3_out, g4_out]\n",
    "\n",
    "# print(\"\\n\\n------------\\nPropagation complete. Propagated cumulants:\")\n",
    "# print(f\" mean: {mu_out} \\n cov: {cov_out}\\n\")\n",
    "# # print(f\" g3: {g3_out} \\n g4: {g4_out}\\n\") # include if using higher cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vals = np.linspace(np.min(network_outputs), np.max(network_outputs), 500)\n",
    "\n",
    "# # Find the pdf using propagated cumulants\n",
    "# pdf_truncated_propagated = theoretical_pdf_normal(y_vals, mu_out, cov_out)\n",
    "# pdf_higher_propagated = pdf_from_cumulants(y_vals, cumulants_plotting, t_max=80, num_points=2**13)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plot_histogram(network_outputs, bins=100, color='orange', show=True, norm=True, edgecolor='none')\n",
    "\n",
    "# # Plot with both overlaid\n",
    "# plot_histogram(network_outputs, bins=100, color='orange', show=False, norm=True)\n",
    "# plt.plot(y_vals, pdf_truncated_propagated, label=\"Gaussian\", color='black', linewidth=2)\n",
    "# plt.plot(y_vals, pdf_higher_propagated, label=\"With $\\kappa_{3}, \\kappa_{4}$\", color='blue', linewidth=2)\n",
    "\n",
    "# # plt.title(\"Empirical Data vs Theoretical PDF\")\n",
    "# plt.xlabel(\"Network Output $y$\")\n",
    "# plt.ylabel(\"Probability Density $p(y)$\")\n",
    "# plt.legend(fontsize=9)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Plot with just truncated\n",
    "# plot_histogram(network_outputs, bins=100, color='orange', show=False, norm=True)\n",
    "# plt.plot(y_vals, pdf_truncated_propagated, label=\"Gaussian\", color='black', linewidth=2)\n",
    "# plt.xlabel(\"Network Output $y$\")\n",
    "# plt.ylabel(\"Probability Density $p(y)$\")\n",
    "# plt.legend(fontsize=9)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot with just higher\n",
    "# plot_histogram(network_outputs, bins=100, color='orange', show=False, norm=True)\n",
    "# plt.plot(y_vals, pdf_higher_propagated, label=\"With $\\kappa_{3}, \\kappa_{4}$\", color='blue', linewidth=2)\n",
    "# plt.xlabel(\"Network Output $y$\")\n",
    "# plt.ylabel(\"Probability Density $p(y)$\")\n",
    "# plt.legend(fontsize=9)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "- When we call propagate_full_gaussian_only() need to be careful of setting biases=None or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency with Analytical Solutions\n",
    "\n",
    "Reminder:\n",
    "- 1 linear layer, no biases: $w_{0} = w_{1} = \\frac{1}{\\sqrt{2}}$\n",
    "- 1 linear layer, with biases: same as above, with $b = 0$\n",
    "- 2 linear layers, no biases: $w_{0} = \\frac{\\sqrt{2}}{2a_{0}} - w_{1}$\n",
    "- 2 linear layers, with biases: $b_{2} = -2a_{0}b_{1}, \\; w_{0} = \\frac{1}{a_{0}\\sqrt(2)} - w_{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0: 0.6517329485472669, sqrt(2)/2a0 - w1: 0.6517329482769905\n"
     ]
    }
   ],
   "source": [
    "weights, biases = extract_weights(model)\n",
    "layer1, layer2 = weights[0], weights[1]\n",
    "w0, w1, w2, w3 = layer1[0, 0], layer1[0, 1], layer1[1, 0], layer1[1, 1]\n",
    "a0, a1 = layer2[0, 0], layer2[0, 1]\n",
    "\n",
    "if biases:\n",
    "    biases1, biases2 = biases[0], biases[1]\n",
    "    b0, b1 = biases1[0], biases1[1]\n",
    "    b2 = biases2[0]\n",
    "\n",
    "print(f\"w0: {w0}, sqrt(2)/2a0 - w1: {np.sqrt(2)/(2 * a0) - w1}\")\n",
    "if biases:\n",
    "    print(f\"b2: {b2}, -2*a0*b1: {-2 * a0 * b1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding KL Divergence Between Theoretical/Empirical Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import rel_entr  # element-wise P * log(P / Q)\n",
    "\n",
    "def norm_kl_div_histtopdf(samples, pdf_vals, y_vals):\n",
    "    \"\"\"\n",
    "    Compute normalized KL divergence: D_KL(P_emp || P_theo) / H(P_emp), normalising by the Shannon entropy.\n",
    "\n",
    "    Use y_vals as pdf evaluation points = bin centres, convert to bin edges.\n",
    "    Converts density into discrete probability mass function (PMF) using histogram.\n",
    "    Normalises the PMF to ensure it sums to 1.\n",
    "    Clipping to avoid log(0) and other numerical issues.\n",
    "\n",
    "    KL divergence is outputted in units of nats, but this isn't a physical unit, rather a measure of relative information between distributions.\n",
    "        This normalised KL divergence is unitless, and ranges from 0 to 1.\n",
    "    0 = perfect match, 1 = divergence as large as the entropy. \"\"\"\n",
    "\n",
    "    # Step 1: Histogram\n",
    "    dy = y_vals[1] - y_vals[0]  # spacing between points\n",
    "    bin_edges = np.linspace(y_vals[0] - dy / 2, y_vals[-1] + dy / 2, len(y_vals) + 1) # ensures that distribs are defined over matching bins (diff histogram to the previous plots with bin=100)\n",
    "\n",
    "    hist, bin_edges = np.histogram(samples, bins=bin_edges, density=True)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "\n",
    "    # Empirical PMF\n",
    "    p_empirical = hist * bin_widths\n",
    "\n",
    "    # Interpolate theoretical PDF to match bin centers\n",
    "    pdf_interp = np.interp(bin_centers, y_vals, pdf_vals)\n",
    "    p_theoretical = pdf_interp * bin_widths\n",
    "    p_theoretical /= np.sum(p_theoretical)\n",
    "\n",
    "    # Clip to avoid numerical issues\n",
    "    eps = 1e-12\n",
    "    p_empirical = np.clip(p_empirical, eps, 1)\n",
    "    p_theoretical = np.clip(p_theoretical, eps, 1)\n",
    "\n",
    "    # KL divergence and entropy\n",
    "    kl = np.sum(rel_entr(p_empirical, p_theoretical))\n",
    "    entropy = -np.sum(p_empirical * np.log(p_empirical))\n",
    "\n",
    "    normalized_kl = kl / entropy\n",
    "    return normalized_kl\n",
    "\n",
    "def norm_kl_div_pdf2pdf(p_vals, q_vals, y_vals, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute normalized KL divergence: D_KL(P || Q) / H(P)\n",
    "\n",
    "    Args:\n",
    "        p_vals (np.ndarray): First PDF values (e.g., empirical).\n",
    "        q_vals (np.ndarray): Second PDF values (e.g., theoretical).\n",
    "        y_vals (np.ndarray): Evaluation points (must be evenly spaced).\n",
    "        eps (float): Small value to avoid log(0) or division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized KL divergence (unitless, range 0–1)\n",
    "    \"\"\"\n",
    "    dy = y_vals[1] - y_vals[0]\n",
    "\n",
    "    # Clip PDFs to avoid log(0) or division by 0\n",
    "    p_safe = np.clip(p_vals, eps, 1)\n",
    "    q_safe = np.clip(q_vals, eps, 1)\n",
    "\n",
    "    # KL divergence and entropy (discretized)\n",
    "    kl = np.sum(rel_entr(p_safe, q_safe)) * dy\n",
    "    entropy = -np.sum(p_safe * np.log(p_safe)) * dy\n",
    "\n",
    "    normalized_kl = kl / entropy\n",
    "    return normalized_kl\n",
    "\n",
    "# First find the theoretical pdf using given set of cumulants\n",
    "\n",
    "# Ground truth\n",
    "gt_cumulants_list_linear = [ground_truth_c['kappa1'], ground_truth_c['kappa2'], ground_truth_c['kappa3'], ground_truth_c['kappa4']]\n",
    "gt_pdf_trunc = theoretical_pdf_normal(y_vals, ground_truth_c['kappa1'], ground_truth_c['kappa2'])\n",
    "gt_pdf_higher = pdf_from_cumulants(y_vals, gt_cumulants_list_linear, t_max=80, num_points=2**13)\n",
    "\n",
    "# JL predictions\n",
    "jl_cumulants_list_linear = [avg_jl_cumulants['kappa1'], avg_jl_cumulants['kappa2'], avg_jl_cumulants['kappa3'], avg_jl_cumulants['kappa4']]\n",
    "jl_pdf_trunc = theoretical_pdf_normal(y_vals, avg_jl_cumulants['kappa1'], avg_jl_cumulants['kappa2'])\n",
    "jl_pdf_higher = pdf_from_cumulants(y_vals, jl_cumulants_list_linear, t_max=80, num_points=2**13)\n",
    "\n",
    "\n",
    "# Find KL divergences\n",
    "\n",
    "# network output/ground truth\n",
    "kl_netout_gt_trunc = norm_kl_div_histtopdf(network_outputs, gt_pdf_trunc, y_vals)\n",
    "kl_netout_gt_higher = norm_kl_div_histtopdf(network_outputs, gt_pdf_higher, y_vals)\n",
    "\n",
    "# # propagated/ground truth\n",
    "# kl_propagated_gt_trunc = norm_kl_div_pdf2pdf(pdf_truncated_propagated, gt_pdf_trunc, y_vals)\n",
    "# kl_propagated_gt_higher = norm_kl_div_pdf2pdf(pdf_higher_propagated, gt_pdf_higher, y_vals)\n",
    "\n",
    "# # network output/propagated\n",
    "# kl_netout_propagated_trunc = norm_kl_div_histtopdf(network_outputs, pdf_truncated_propagated, y_vals)\n",
    "# kl_netout_propagated_higher = norm_kl_div_histtopdf(network_outputs, pdf_higher_propagated, y_vals)\n",
    "\n",
    "# JL predictions/ground truth\n",
    "kl_jl_gt_trunc = norm_kl_div_pdf2pdf(jl_pdf_trunc, gt_pdf_trunc, y_vals)\n",
    "kl_jl_gt_higher = norm_kl_div_pdf2pdf(jl_pdf_higher, gt_pdf_higher, y_vals)\n",
    "\n",
    "# network output/JL predictions\n",
    "kl_netout_jl_trunc = norm_kl_div_histtopdf(network_outputs, jl_pdf_trunc, y_vals)\n",
    "kl_netout_jl_higher = norm_kl_div_histtopdf(network_outputs, jl_pdf_higher, y_vals)\n",
    "\n",
    "\n",
    "# # Test\n",
    "# baseline = pdf_from_cumulants(y_vals, gt_cumulants_list_linear, t_max=50, num_points=2**12)\n",
    "# high_res = pdf_from_cumulants(y_vals, gt_cumulants_list_linear, t_max=80, num_points=2**13)\n",
    "# super_high = pdf_from_cumulants(y_vals, gt_cumulants_list_linear, t_max=200, num_points=2**14)\n",
    "# kl_test = norm_kl_div_pdf2pdf(super_high, high_res, y_vals)\n",
    "# print(kl_test)\n",
    "# plt.plot(y_vals, baseline, label=\"baseline\", linewidth=2)\n",
    "# plt.plot(y_vals, high_res, label=\"high res\", linewidth=2)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(y_vals, baseline - high_res)\n",
    "# plt.title(\"PDF Difference (baseline - high_res)\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def wasserstein_histtopdf(samples, pdf_vals, y_vals):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein distance between empirical samples and a theoretical PDF evaluated on y_vals.\n",
    "\n",
    "    Args:\n",
    "        samples (np.ndarray): Empirical samples.\n",
    "        pdf_vals (np.ndarray): Theoretical PDF values (must match y_vals).\n",
    "        y_vals (np.ndarray): Points at which PDF is evaluated (assumed evenly spaced).\n",
    "\n",
    "    Returns:\n",
    "        float: Wasserstein distance (same units as y_vals)\n",
    "    \"\"\"\n",
    "    dy = y_vals[1] - y_vals[0]\n",
    "\n",
    "    # Create a sample-based approximation of the PDF (discrete measure)\n",
    "    theoretical_mass = pdf_vals * dy\n",
    "    theoretical_mass /= np.sum(theoretical_mass)  # normalize to sum to 1\n",
    "\n",
    "    # Treat y_vals as support and theoretical_mass as weights\n",
    "    return wasserstein_distance(u_values=samples, v_values=y_vals, v_weights=theoretical_mass)\n",
    "\n",
    "def wasserstein_pdf2pdf(p_vals, q_vals, y_vals):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein distance between two PDFs evaluated on a shared grid.\n",
    "\n",
    "    Args:\n",
    "        p_vals (np.ndarray): PDF 1 (e.g., empirical).\n",
    "        q_vals (np.ndarray): PDF 2 (e.g., theoretical).\n",
    "        y_vals (np.ndarray): Evaluation points (assumed evenly spaced).\n",
    "\n",
    "    Returns:\n",
    "        float: Wasserstein distance\n",
    "    \"\"\"\n",
    "    dy = y_vals[1] - y_vals[0]\n",
    "\n",
    "    # Normalize both PDFs to sum to 1\n",
    "    p_mass = p_vals * dy\n",
    "    q_mass = q_vals * dy\n",
    "    p_mass /= np.sum(p_mass)\n",
    "    q_mass /= np.sum(q_mass)\n",
    "\n",
    "    return wasserstein_distance(u_values=y_vals, v_values=y_vals, u_weights=p_mass, v_weights=q_mass)\n",
    "\n",
    "\n",
    "# Find the distances\n",
    "\n",
    "# network output/ground truth\n",
    "wd_netout_gt_trunc = norm_kl_div_histtopdf(network_outputs, gt_pdf_trunc, y_vals)\n",
    "wd_netout_gt_higher = norm_kl_div_histtopdf(network_outputs, gt_pdf_higher, y_vals)\n",
    "\n",
    "# # propagated/ground truth\n",
    "# wd_propagated_gt_trunc = norm_kl_div_pdf2pdf(pdf_truncated_propagated, gt_pdf_trunc, y_vals)\n",
    "# wd_propagated_gt_higher = norm_kl_div_pdf2pdf(pdf_higher_propagated, gt_pdf_higher, y_vals)\n",
    "\n",
    "# # network output/propagated\n",
    "# wd_netout_propagated_trunc = norm_kl_div_histtopdf(network_outputs, pdf_truncated_propagated, y_vals)\n",
    "# wd_netout_propagated_higher = norm_kl_div_histtopdf(network_outputs, pdf_higher_propagated, y_vals)\n",
    "\n",
    "# JL predictions/ground truth\n",
    "wd_jl_gt_trunc = norm_kl_div_pdf2pdf(jl_pdf_trunc, gt_pdf_trunc, y_vals)\n",
    "wd_jl_gt_higher = norm_kl_div_pdf2pdf(jl_pdf_higher, gt_pdf_higher, y_vals)\n",
    "\n",
    "# network output/JL predictions\n",
    "wd_netout_jl_trunc = norm_kl_div_histtopdf(network_outputs, jl_pdf_trunc, y_vals)\n",
    "wd_netout_jl_higher = norm_kl_div_histtopdf(network_outputs, jl_pdf_higher, y_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulant Deviation\n",
    "\n",
    "Output the % deviation of propagated from reference (Jona-Lasinio values of) cumulants: $$\\text{Relative Error} = \\frac{\\left| k_n^{\\text{emp}} - k_n^{\\text{theo}} \\right|}{\\left| k_n^{\\text{theo}} \\right|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # avg_input_c, z_dict = find_input_cumulants(train_samples)\n",
    "# # avg_jl_cumulants, _ = find_pred_output_cumulants(train_samples)\n",
    "# # network_outputs, network_cumulants = find_network_output_cumulants(trainloader, model)\n",
    "\n",
    "# # # Networks with nonlinearity\n",
    "# # # mu_out, cov_out = propagate_full_gaussian_only(z_dict, weights, biases=None) # if using biases, set biases = True\n",
    "\n",
    "# # # Networks without nonlinearity\n",
    "# # mu_out, cov_out, g3_out, g4_out = propagate_full_linear(z_dict, weights, biases=None) # if using biases, set biases = True\n",
    "# # cumulants_plotting = [mu_out, cov_out, g3_out, g4_out]\n",
    "\n",
    "# def compare_cumulants_all(propagated, reference, orders=None, relative=False):\n",
    "#     \"\"\"\n",
    "#     Compare propagated vs reference cumulants.\n",
    "\n",
    "#     Args:\n",
    "#         propagated (list): List of tensors (possibly nested) for κ₁, κ₂, ...\n",
    "#         reference (dict): Dictionary with keys 'kappa1', 'kappa2', ... as floats or tensors.\n",
    "#         orders (list): Orders to compare (e.g., [1, 2, 3, 4])\n",
    "#         relative (bool): If True, compute relative error.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: {n: error} for each cumulant order\n",
    "#     \"\"\"\n",
    "#     errors = {}\n",
    "#     epsilon = torch.finfo(torch.float32).eps\n",
    "\n",
    "#     if orders is None:\n",
    "#         orders = [int(k.replace('kappa', '')) for k in reference if k.startswith('kappa')]\n",
    "\n",
    "#     for n in orders:\n",
    "#         k_prop = propagated[n - 1]\n",
    "#         k_ref = reference[f'kappa{n}']\n",
    "\n",
    "#         # Convert to scalar\n",
    "#         if hasattr(k_prop, 'item'):\n",
    "#             k_prop = k_prop.item()\n",
    "#         # elif hasattr(k_prop, 'numpy') and k_prop.size == 1:\n",
    "#         #     k_prop = float(k_prop)\n",
    "\n",
    "#         if hasattr(k_ref, 'item'):\n",
    "#             k_ref = k_ref.item()\n",
    "\n",
    "#         # Compute error\n",
    "#         abs_error = abs(k_prop - k_ref)\n",
    "#         if relative:\n",
    "#             # denom = abs(k_ref) if abs(k_ref) > 1e-12 else 1e-12\n",
    "#             denom = abs(k_ref) + epsilon\n",
    "#             errors[n] = abs_error * 100 / denom # relative error in %\n",
    "#         else:\n",
    "#             errors[n] = abs_error # absolute error\n",
    "\n",
    "#     return errors\n",
    "\n",
    "# # First print the raw cumulants to save\n",
    "# print(\"Average input cumulants, across both columns:\\n\")\n",
    "# for k, v in avg_input_c.items():\n",
    "#     print(f\"{k}: {v}\")\n",
    "\n",
    "# print(\"\\nAverage predicted output cumulants (JL)L\\n\")\n",
    "# for k, v in avg_jl_cumulants.items():\n",
    "#     print(f\"{k}: {v}\")\n",
    "\n",
    "# print(\"\\nEmpirical network output cumulants:\\n\")\n",
    "# for k, v in network_cumulants.items():\n",
    "#     print(f\"{k}: {v}\")\n",
    "\n",
    "# print(\"\\nPropagated theoretical cumulants:\\n\")\n",
    "# for i in range(len(cumulants_plotting)):\n",
    "#     print(f\"kappa{i+1}: {cumulants_plotting[i]}\")\n",
    "\n",
    "# # Print the deviations\n",
    "\n",
    "# # Empirical network output v. JL predictions\n",
    "# net_cumulants_list = [network_cumulants['kappa1'], network_cumulants['kappa2'], network_cumulants['kappa3'], network_cumulants['kappa4']]\n",
    "# emp_errors_rel = compare_cumulants_all(net_cumulants_list, avg_jl_cumulants, orders=[1, 2, 3, 4], relative=True)\n",
    "# emp_errors_abs = compare_cumulants_all(net_cumulants_list, avg_jl_cumulants, orders=[1, 2, 3, 4], relative=False)\n",
    "# print(\"\\nEmpirical network output v JL predictions:\\n Cumulant errors, relative %:\\n\")\n",
    "# for k, v in emp_errors_rel.items():\n",
    "#     print(f\"Order {k}: {v}\")\n",
    "# print(\"\\n Cumulant errors, absolute:\\n\")\n",
    "# for k, v in emp_errors_abs.items():\n",
    "#     print(f\"Order {k}: {v}\")\n",
    "# print(\"\\n------------------\")\n",
    "\n",
    "# # Propagated v. JL predictions\n",
    "# prop_errors_rel = compare_cumulants_all(cumulants_plotting, avg_jl_cumulants, orders=[1, 2, 3, 4], relative=True)\n",
    "# prop_errors_abs = compare_cumulants_all(cumulants_plotting, avg_jl_cumulants, orders=[1, 2, 3, 4], relative=False)\n",
    "# print(\"\\nPropagated v JL predictions:\\n Cumulant errors, relative %:\\n\")\n",
    "# for k, v in prop_errors_rel.items():\n",
    "#     print(f\"Order {k}: {v}\")\n",
    "# print(\"\\n Cumulant errors, absolute:\\n\")\n",
    "# for k, v in prop_errors_abs.items():\n",
    "#     print(f\"Order {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "def find_next_empty_row(ws, anchor_col=2):\n",
    "    row = 6\n",
    "    while ws.cell(row=row, column=anchor_col).value is not None:\n",
    "        row += 1\n",
    "    return row\n",
    "\n",
    "# Load your workbook and sheet\n",
    "wb = load_workbook(r\"C:\\Users\\cga28\\Documents\\Project\\new_results.xlsx\")\n",
    "ws = wb[\"symm relu\"]  # or the exact name of your sheet\n",
    "\n",
    "# Example: write raw cumulants starting at column H (8), row 3\n",
    "# start_row = 12\n",
    "start_row = find_next_empty_row(ws)\n",
    "start_col = 2 #(B)\n",
    "cumulant_start_column = 7\n",
    "KL_start_column = 27\n",
    "wass_start_column = 38\n",
    "dead_start_column = 49\n",
    "\n",
    "ws.cell(row=start_row, column=start_col, value=var)\n",
    "ws.cell(row=start_row, column=start_col - 1, value=SEED)\n",
    "\n",
    "# Write average input cumulants (1st row)\n",
    "for i, val in enumerate(avg_input_c.values()):\n",
    "    ws.cell(row=start_row, column=cumulant_start_column + i, value=float(val))\n",
    "\n",
    "# Write JL prediction cumulants\n",
    "for i, val in enumerate(avg_jl_cumulants.values()):\n",
    "    ws.cell(row=start_row, column=cumulant_start_column + 4 + i, value=float(val))\n",
    "\n",
    "# Write empirical cumulants\n",
    "for i, val in enumerate(network_cumulants.values()):\n",
    "    ws.cell(row=start_row, column=cumulant_start_column + 8 + i, value=float(val))\n",
    "\n",
    "# # Write propagated cumulants\n",
    "# for i, val in enumerate(cumulants_plotting):\n",
    "#     ws.cell(row=start_row, column=cumulant_start_column + 12 + i, value=float(val))\n",
    "\n",
    "# Write ground truth cumulants\n",
    "for i, val in enumerate(ground_truth_c.values()):\n",
    "    ws.cell(row=start_row, column=cumulant_start_column + 16 + i, value=float(val))\n",
    "\n",
    "\n",
    "ws.cell(row=start_row, column=KL_start_column, value=kl_netout_gt_trunc)\n",
    "ws.cell(row=start_row, column=KL_start_column+1, value=kl_netout_gt_higher)\n",
    "\n",
    "# ws.cell(row=start_row, column=KL_start_column+2, value=kl_propagated_gt_trunc)\n",
    "# ws.cell(row=start_row, column=KL_start_column+3, value=kl_propagated_gt_higher)\n",
    "\n",
    "# ws.cell(row=start_row, column=KL_start_column+4, value=kl_netout_propagated_trunc)\n",
    "# ws.cell(row=start_row, column=KL_start_column+5, value=kl_netout_propagated_higher)\n",
    "\n",
    "ws.cell(row=start_row, column=KL_start_column+6, value=kl_jl_gt_trunc)\n",
    "ws.cell(row=start_row, column=KL_start_column+7, value=kl_jl_gt_higher)\n",
    "\n",
    "ws.cell(row=start_row, column=KL_start_column+8, value=kl_netout_jl_trunc)\n",
    "ws.cell(row=start_row, column=KL_start_column+9, value=kl_netout_jl_higher)\n",
    "\n",
    "# Write wasserstein distances\n",
    "ws.cell(row=start_row, column=wass_start_column, value=wd_netout_gt_trunc)\n",
    "ws.cell(row=start_row, column=wass_start_column+1, value=wd_netout_gt_higher)\n",
    "\n",
    "# ws.cell(row=start_row, column=wass_start_column+2, value=wd_propagated_gt_trunc)\n",
    "# ws.cell(row=start_row, column=wass_start_column+3, value=wd_propagated_gt_higher)\n",
    "\n",
    "# ws.cell(row=start_row, column=wass_start_column+4, value=wd_netout_propagated_trunc)\n",
    "# ws.cell(row=start_row, column=wass_start_column+5, value=wd_netout_propagated_higher)\n",
    "\n",
    "ws.cell(row=start_row, column=wass_start_column+6, value=wd_jl_gt_trunc)\n",
    "ws.cell(row=start_row, column=wass_start_column+7, value=wd_jl_gt_higher)\n",
    "\n",
    "ws.cell(row=start_row, column=wass_start_column+8, value=wd_netout_jl_trunc)\n",
    "ws.cell(row=start_row, column=wass_start_column+9, value=wd_netout_jl_higher)\n",
    "\n",
    "\n",
    "# Write each neuron's zero-output fraction to Excel\n",
    "for i, frac in enumerate(zero_fractions):\n",
    "    ws.cell(row=start_row, column=dead_start_column + i, value=frac)\n",
    "\n",
    "# Save total as one extra column\n",
    "ws.cell(row=start_row, column=dead_start_column + n_neurons, value=total_dead_frac)\n",
    "\n",
    "\n",
    "# Save workbook\n",
    "wb.save(r\"C:\\Users\\cga28\\Documents\\Project\\new_results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating the CLT RG Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class duplicated_oneDData(Dataset):\n",
    "    def __init__(self, inputs, ground_outputs):\n",
    "        self.samples, self.true_samples = inputs, ground_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "    \n",
    "\n",
    "def get_iterated_data(model, dataloader):\n",
    "    \"\"\" This function performs an iteration of the model on the data. It then produces a duplicated dataset.\"\"\"\n",
    "\n",
    "    outputs_np = collect_model_outputs(dataloader, model) # (N,)\n",
    "    N = outputs_np.shape[0]\n",
    "    shufffled_outputs = np.random.permutation(outputs_np)  # Shuffle the outputs\n",
    "    new_inputs = np.column_stack((outputs_np, shufffled_outputs))  # Shape: (N, 2)\n",
    "    # Perform decimation as in your get_data function\n",
    "    decimated_outputs = np.sum(new_inputs, axis=1) / np.sqrt(2)  # Shape: (N,)\n",
    "    \n",
    "    iterated_data = duplicated_oneDData(new_inputs, decimated_outputs)\n",
    "\n",
    "    return iterated_data\n",
    "\n",
    "num_iterations = 1\n",
    "current_loader = testloader\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    iterated_data = get_iterated_data(model, current_loader)\n",
    "    current_loader = DataLoader(iterated_data, batch_size=64, shuffle=False)\n",
    "\n",
    "iterated_outputs = collect_model_outputs(current_loader, model) # now we have n+1 iterations\n",
    "plot_histogram(iterated_outputs, bins=100, color='orange', label=f'{num_iterations+1} Iterations', legend=True, norm=True, show=False)\n",
    "plt.xlabel(\"Iterated Network Output $y$\")\n",
    "plt.ylabel(\"Probability Density $p(y)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated Cumulant Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Empirical Cumulants (across both input columns):\n",
      "kappa1: -0.0019374754678025553\n",
      "kappa2: 8.008635164913134\n",
      "kappa3: -0.04281856082451299\n",
      "kappa4: 0.20113099301671866\n",
      "\n",
      "Predicted Cumulants (after iterating):\n",
      "kappa1: -0.0038749509356051114\n",
      "kappa2: 8.008635164913134\n",
      "kappa3: -0.0214092804122565\n",
      "kappa4: 0.050282748254179666\n",
      "\n",
      "Extracted Cumulants (of iterated network output):\n",
      "kappa1: 1.5955232386044313\n",
      "kappa2: 2.7290775029535252\n",
      "kappa3: 5.224576180629225\n",
      "kappa4: 8.85274757658086\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compare_cumulants_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[631], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Compare deviation from expected cumulants\u001b[39;00m\n\u001b[0;32m     57\u001b[0m iterated_cumulants_list \u001b[38;5;241m=\u001b[39m [iterated_cumulants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa1\u001b[39m\u001b[38;5;124m'\u001b[39m], iterated_cumulants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa2\u001b[39m\u001b[38;5;124m'\u001b[39m], iterated_cumulants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa3\u001b[39m\u001b[38;5;124m'\u001b[39m], iterated_cumulants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkappa4\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 59\u001b[0m it_errors_rel \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_cumulants_all\u001b[49m(iterated_cumulants_list, predicted_cumulants, orders\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], relative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m it_errors_abs \u001b[38;5;241m=\u001b[39m compare_cumulants_all(iterated_cumulants_list, predicted_cumulants, orders\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], relative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCumulant Errors, rel (%): \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit_errors_rel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compare_cumulants_all' is not defined"
     ]
    }
   ],
   "source": [
    "def JL_cumulant_evolution(data, num_iterations):\n",
    "    \"\"\" Input: data is a dictionary of cumulants, num_iterations is the number of iterations to perform.\n",
    "        Output: a dictionary of cumulants after num_iterations iterations.\n",
    "        \n",
    "        The functions scales each nth order cumulant by the appropriate factor for the number of iterations.\"\"\"\n",
    "    \n",
    "    scaled_cumulants = {}\n",
    "    for key, value in data.items():\n",
    "        # Extract the cumulant order (k) from the key string 'kappa{k}'\n",
    "        k = int(key[-1])\n",
    "        \n",
    "        # Calculate the scaling factor\n",
    "        scaling_factor = 2 ** (1 - k / 2)\n",
    "        \n",
    "        # Scale the cumulant value num_iterations times\n",
    "        scaled_cumulants[key] = value * (scaling_factor ** (num_iterations + 1))\n",
    "    \n",
    "    return scaled_cumulants\n",
    "\n",
    "# Expected JL cumulants - first must extract input cumulants\n",
    "current_samples = test_samples\n",
    "\n",
    "first_col = []\n",
    "second_col = []\n",
    "labels = []\n",
    "for i in range(len(current_samples)):\n",
    "    x, y = current_samples[i]\n",
    "    first_col.append(x[0].item())\n",
    "    second_col.append(x[1].item())\n",
    "    labels.append(y.item())\n",
    "\n",
    "first_col = np.array(first_col)  # Shape: (n_train, )\n",
    "second_col = np.array(second_col)  # Shape: (n_train, )\n",
    "labels = np.array(labels)  # Shape: (n_train, )\n",
    "\n",
    "col1 = cumulant_extraction(first_col)\n",
    "col2 = cumulant_extraction(second_col)\n",
    "avg_initial_cumulants = {}\n",
    "for k in col1.keys():\n",
    "    avg_initial_cumulants[k] = 0.5 * (col1[k] + col2[k])\n",
    "print(\"Average Empirical Cumulants (across both input columns):\")\n",
    "for k, v in avg_initial_cumulants.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "predicted_cumulants = JL_cumulant_evolution(avg_initial_cumulants, num_iterations)\n",
    "print(\"\\nPredicted Cumulants (after iterating):\")\n",
    "for k, v in predicted_cumulants.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Find cumulants of iterated outputs\n",
    "iterated_cumulants = cumulant_extraction(iterated_outputs)\n",
    "print(f\"\\nExtracted Cumulants (of iterated network output):\")\n",
    "for k, v in iterated_cumulants.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Compare deviation from expected cumulants\n",
    "iterated_cumulants_list = [iterated_cumulants['kappa1'], iterated_cumulants['kappa2'], iterated_cumulants['kappa3'], iterated_cumulants['kappa4']]\n",
    "\n",
    "it_errors_rel = compare_cumulants_all(iterated_cumulants_list, predicted_cumulants, orders=[1, 2, 3, 4], relative=True)\n",
    "it_errors_abs = compare_cumulants_all(iterated_cumulants_list, predicted_cumulants, orders=[1, 2, 3, 4], relative=False)\n",
    "print(f\"\\nCumulant Errors, rel (%): \\n {it_errors_rel}\")\n",
    "print(f\"\\nCumulant Errors, abs: \\n {it_errors_abs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Note that currently, this is finding the deviation from empirical network output cumulants v predicted cumulants.\n",
    "\n",
    "The previous deviation section is finding the deviation between propagated cumulants and the predicted cumulants (will become much worse in the gaussian approximation for the nonlinearity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
