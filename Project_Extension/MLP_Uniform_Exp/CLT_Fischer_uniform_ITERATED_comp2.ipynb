{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLT\n",
    "\n",
    "This script correctly find the weights such that the network learns the deterministic map $\\zeta_{n+1} = \\frac{1}{\\sqrt{2}} (\\zeta_n^1 + \\zeta_n^2)$. Let's choose to consider the uniform distribution, and use MSE loss.\n",
    "\n",
    "We attempt to find the network mapping of cumulants ('the statistical model') in this Gaussian approximation, and compare it to the analytical 'true' cumulant transformation that we know that the CLT RG decimation induces. Cumulants should transform individually in a way that is consistent with Fischer paper's framework under these weights. Where nonlinearities are used, this notebook uses the Fischer approach of only considering the network as propagating Gaussian statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import norm\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\"],\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"legend.frameon\": False,\n",
    "    # \"font.size\": 11,\n",
    "    # \"axes.labelsize\": 11,\n",
    "    # \"xtick.labelsize\": 10,\n",
    "    # \"ytick.labelsize\": 10,\n",
    "    # \"legend.fontsize\": 10,\n",
    "})\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 45\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "Here we consider a uniform distribution.\n",
    "\n",
    "First define some useful functions to calculate cumulants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulant_extraction(data):\n",
    "    \"\"\"\n",
    "    Compute the first four cumulants for a 1D array.\n",
    "    Parameters:\n",
    "        data: np.ndarray of shape (N,)\n",
    "    Returns:\n",
    "        A dictionary with cumulants k1 to k4\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)                          # κ1\n",
    "    centered = data - mean\n",
    "\n",
    "    var = np.mean(centered**2)                    # κ2\n",
    "    third = np.mean(centered**3)                  # κ3\n",
    "    fourth_raw = np.mean(centered**4)\n",
    "    fourth = fourth_raw - 3 * var**2              # κ4\n",
    "\n",
    "    return {\n",
    "        'kappa1': mean,\n",
    "        'kappa2': var,\n",
    "        'kappa3': third,\n",
    "        'kappa4': fourth\n",
    "    }\n",
    "\n",
    "def expected_cumulant_evolution(data):\n",
    "    \"\"\"Input: data is a dictionary of input cumulants\n",
    "    This function scales each cumulant according to expected transform from the Jona-Lasinio paper.\"\"\"\n",
    "    \n",
    "    scaled_cumulants = {}\n",
    "    for key, value in data.items():\n",
    "        # Extract the cumulant order (k) from the key string 'kappa{k}'\n",
    "        k = int(key[-1])\n",
    "        \n",
    "        # Calculate the scaling factor\n",
    "        scaling_factor = 2 ** (1 - k / 2)\n",
    "        \n",
    "        # Scale the cumulant value\n",
    "        scaled_cumulants[key] = value * scaling_factor\n",
    "    \n",
    "    return scaled_cumulants\n",
    "\n",
    "import itertools\n",
    "def symmetrise_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Symmetrise a tensor over all its axes.\n",
    "    Args:\n",
    "        tensor (np.ndarray): A tensor of shape (d, d, ..., d)\n",
    "    Returns:\n",
    "        sym_tensor (np.ndarray): Symmetrised version of the input tensor.\n",
    "    \"\"\"\n",
    "    order = tensor.ndim\n",
    "    perms = list(itertools.permutations(range(order)))\n",
    "    sym_tensor = sum(np.transpose(tensor, axes=perm) for perm in perms)\n",
    "    sym_tensor /= len(perms)\n",
    "    return sym_tensor\n",
    "\n",
    "def cumulant_extraction_multidim(data):\n",
    "    \"\"\"\n",
    "    Compute the first four cumulants for a 2D array.\n",
    "    Parameters:\n",
    "        data: np.ndarray of shape (N, 2)\n",
    "    Returns:\n",
    "        A dictionary with cumulants k1 to k4\n",
    "    \"\"\"\n",
    "    N, d = data.shape\n",
    "    \n",
    "    # Mean vector (κ1)\n",
    "    mean = np.mean(data, axis=0)  # Shape: (2,)\n",
    "    \n",
    "    # Centered data\n",
    "    centered = data - mean  # Shape: (N, 2)\n",
    "    \n",
    "    # Covariance matrix (κ2)\n",
    "    \" Using einsum() to find the covariance is implicitly assuming that there is already some level of correlation\"\n",
    "    \" Between the two dimensions. If the data is uncorrelated, this will yield a covariance matrix with off-diagonal elements close to zero.\"\n",
    "    \" But the data should be two independent columns initially, with a diagonal covariance - and correlations will emerge as you propagate them.\"\n",
    "    \n",
    "    \" We shouldn't manually diagonalise the covariance.\"\n",
    "    \n",
    "    # covariance = np.einsum('ni,nj->ij', centered, centered) / N  # Shape: (2, 2)\n",
    "    # covariance = np.diag(np.var(data, axis=0))  # Shape: (2, 2)\n",
    "\n",
    "    # κ2: Covariance matrix\n",
    "    covariance = np.einsum('ni,nj->ij', centered, centered) / N  # Shape: (2, 2)\n",
    "    \n",
    "    # Third-order cumulant tensor (κ3)\n",
    "    G3_raw = np.einsum('ni,nj,nk->ijk', centered, centered, centered) / N  # Shape: (2, 2, 2)\n",
    "    G3 = symmetrise_tensor(G3_raw)\n",
    "    \n",
    "    # Fourth-order cumulant tensor (κ4)\n",
    "    G4_raw = np.einsum('ni,nj,nk,nl->ijkl', centered, centered, centered, centered) / N  # Shape: (2, 2, 2, 2)\n",
    "    \n",
    "    # # Applying the correction for the fourth cumulant\n",
    "    # kappa4 = G4_raw - 3 * np.einsum('ij,kl->ijkl', covariance, covariance) # only valid for jointly gaussian data\n",
    "    \n",
    "    # Full Gaussian correction for fourth cumulant\n",
    "    cov = covariance\n",
    "    term1 = np.einsum('ij,kl->ijkl', cov, cov)\n",
    "    term2 = np.einsum('ik,jl->ijkl', cov, cov)\n",
    "    term3 = np.einsum('il,jk->ijkl', cov, cov)\n",
    "    kappa4_raw = G4_raw - (term1 + term2 + term3)\n",
    "    kappa4 = symmetrise_tensor(kappa4_raw)  # Shape: (2, 2, 2, 2)\n",
    "    \n",
    "    # Package the results in a dictionary\n",
    "    cumulants = {\n",
    "        'kappa1': torch.tensor(mean, dtype=torch.float64),                     # Shape: (2,)\n",
    "        'kappa2': torch.tensor(covariance, dtype=torch.float64),               # Shape: (2, 2)\n",
    "        'kappa3': torch.tensor(G3, dtype=torch.float64),                       # Shape: (2, 2, 2)\n",
    "        'kappa4': torch.tensor(kappa4, dtype=torch.float64)                        # Shape: (2, 2, 2, 2)\n",
    "    }\n",
    "    \n",
    "    return cumulants\n",
    "\n",
    "# Find input cumulants:\n",
    "def find_input_cumulants(dataset):\n",
    "\n",
    "\n",
    "    # Extract inputs and labels from dataset\n",
    "    xs = np.array([x.numpy() for x, _ in dataset])  # shape (N, 2)\n",
    "    \n",
    "    # Compute column-wise cumulants\n",
    "    col1 = cumulant_extraction(xs[:, 0])\n",
    "    col2 = cumulant_extraction(xs[:, 1])\n",
    "    avg_cumulants = {k: 0.5 * (col1[k] + col2[k]) for k in col1}\n",
    "\n",
    "    # Full multivariate cumulants\n",
    "    z_dict = cumulant_extraction_multidim(xs)\n",
    "\n",
    "\n",
    "    # # Of each column:\n",
    "    # first_col = []\n",
    "    # second_col = []\n",
    "    # labels = []\n",
    "    # for i in range(len(data_samples)):\n",
    "    #     x, y = data_samples[i]\n",
    "    #     first_col.append(x[0].item())\n",
    "    #     second_col.append(x[1].item())\n",
    "    #     labels.append(y.item())\n",
    "\n",
    "    # first_col = np.array(first_col)  # Shape: (n_train, )\n",
    "    # second_col = np.array(second_col)  # Shape: (n_train, )\n",
    "    # labels = np.array(labels)  # Shape: (n_train, )\n",
    "\n",
    "    # # Find average input cumulants for each column\n",
    "    # col1 = cumulant_extraction(first_col)\n",
    "    # print(\"\\nInitial Empirical Cumulants (first col):\")\n",
    "    # for k, v in col1.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "    # col2 = cumulant_extraction(second_col)\n",
    "    # print(\"\\nInitial Empirical Cumulants (second col):\")\n",
    "    # for k, v in col2.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # # Find average input cumulants across the two columns\n",
    "    # avg_cumulants = {}\n",
    "    # for k in col1.keys():\n",
    "    #     avg_cumulants[k] = 0.5 * (col1[k] + col2[k])\n",
    "\n",
    "    # print(\"Average Empirical Cumulants (across both input columns):\")\n",
    "    # for k, v in avg_cumulants.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "    # # # Compute the cumulants for the input data (multivariate)\n",
    "    # input_array = data_samples.samples\n",
    "    # z_dict = cumulant_extraction_multidim(input_array)\n",
    "\n",
    "    # print(\"\\nCumulants of Input Data:\")\n",
    "    # for k, v in z_dict.items():\n",
    "    #     print(f\"{k}: shape = {v.shape}, value = {v}\")\n",
    "\n",
    "    return avg_cumulants, z_dict\n",
    "\n",
    "# Compute output cumulants\n",
    "def find_pred_output_cumulants(dataset):\n",
    "\n",
    "    #  # Of each column:\n",
    "    # first_col = []\n",
    "    # second_col = []\n",
    "    # labels = []\n",
    "    # for i in range(len(data_samples)):\n",
    "    #     x, y = data_samples[i]\n",
    "    #     first_col.append(x[0].item())\n",
    "    #     second_col.append(x[1].item())\n",
    "    #     labels.append(y.item())\n",
    "\n",
    "    # first_col = np.array(first_col)  # Shape: (n_train, )\n",
    "    # second_col = np.array(second_col)  # Shape: (n_train, )\n",
    "    # labels = np.array(labels)  # Shape: (n_train, )\n",
    "\n",
    "    # # Jona-Lasinio transform, averaged\n",
    "    # col1 = cumulant_extraction(first_col)\n",
    "    # col2 = cumulant_extraction(second_col)\n",
    "    # expected_cumulants_col1 = expected_cumulant_evolution(col1)\n",
    "    # expected_cumulants_col2 = expected_cumulant_evolution(col2)\n",
    "    # avg_evolved_cumulants = {}\n",
    "    # for k in expected_cumulants_col1.keys():\n",
    "    #     avg_evolved_cumulants[k] = 0.5 * (expected_cumulants_col1[k] + expected_cumulants_col2[k])\n",
    "\n",
    "    # print(\"\\nAverage expected evolved cumulants (Jona-Lasinio), across both columns:\\n\")\n",
    "    # for k, v in avg_evolved_cumulants.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "    # # Ground truth labels\n",
    "    # ground_truth_cumulants = cumulant_extraction(labels)\n",
    "    # print(\"\\nCumulants of the ground truth labels:\")\n",
    "    # for k, v in ground_truth_cumulants.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "    # print(\"-------------------\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    xs = np.array([x.numpy() for x, _ in dataset])\n",
    "    ys = np.array([y.item() for _, y in dataset])\n",
    "\n",
    "    col1 = cumulant_extraction(xs[:, 0])\n",
    "    col2 = cumulant_extraction(xs[:, 1])\n",
    "\n",
    "    evolved1 = expected_cumulant_evolution(col1)\n",
    "    evolved2 = expected_cumulant_evolution(col2)\n",
    "    avg_evolved_cumulants = {k: 0.5 * (evolved1[k] + evolved2[k]) for k in evolved1}\n",
    "\n",
    "    ground_truth_cumulants = cumulant_extraction(ys)\n",
    "\n",
    "    return avg_evolved_cumulants, ground_truth_cumulants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section is now defunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample pairs of points from a 1D uniform distribution\n",
    "# def adjust_samples_to_match_cumulants(y, target_cumulants):\n",
    "#     \"\"\"Adjust distribution to match desired cumulants (preserve mean).\"\"\"\n",
    "#     y_centered = y - np.mean(y)\n",
    "#     y_scaled = y_centered / np.std(y)\n",
    "\n",
    "#     # Use Gram-Charlier-like polynomial correction (not perfect, but practical)\n",
    "#     skew_target = target_cumulants['kappa3'] / (target_cumulants['kappa2'] ** 1.5)\n",
    "#     kurt_target = target_cumulants['kappa4'] / (target_cumulants['kappa2'] ** 2)\n",
    "\n",
    "\n",
    "#     # Fit using a 3rd/4th order polynomial transform to match shape (approximate)\n",
    "#     corrected = y_scaled \\\n",
    "#         + 0.5 * skew_target * (y_scaled**2 - 1) \\\n",
    "#         + (1/24) * kurt_target * (y_scaled**3 - 3*y_scaled)\n",
    "\n",
    "#     # Rescale to target mean and std\n",
    "#     corrected = corrected * np.sqrt(target_cumulants['kappa2']) + target_cumulants['kappa1']\n",
    "#     return corrected\n",
    "\n",
    "# def get_data(number):\n",
    "#     x = np.random.uniform(low=-0.5, high=0.5, size=(number, 2))\n",
    "#     decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "\n",
    "#     first_col = x[:, 0]\n",
    "#     second_col = x[:, 1]\n",
    "\n",
    "#     # Find input cumulants\n",
    "#     col1 = cumulant_extraction(first_col)\n",
    "#     col2 = cumulant_extraction(second_col)\n",
    "#     avg_cumulants = {}\n",
    "#     for k in col1.keys():\n",
    "#         avg_cumulants[k] = 0.5 * (col1[k] + col2[k])\n",
    "#     expected_y_cumulants = expected_cumulant_evolution(avg_cumulants)\n",
    "\n",
    "#     decimated_x_corrected = adjust_samples_to_match_cumulants(decimated_x, expected_y_cumulants)\n",
    "#     return x, decimated_x_corrected\n",
    "\n",
    "# # Check\n",
    "# test_in, test_out = get_data(1000)\n",
    "# for i in range(len(test_in[:, 0])):\n",
    "#     if test_out[i] != np.sum(test_in[i]) / np.sqrt(2):\n",
    "#         print(\"Error in cumulant matching\")\n",
    "#         break\n",
    "\n",
    "# test_out_cumulants = cumulant_extraction(test_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data - Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pairs of points from a 1D uniform distribution\n",
    "def get_data_uniform(number):\n",
    "    x = np.random.uniform(low=0, high=1, size=(number, 2))\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "def get_data_gaussian(number): # centred unit gaussian\n",
    "    x = np.random.normal(loc=0, scale=1.0, size=(number, 2)) # loc is mean, scale is stdev (not variance)\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "def get_data_gaussian_shifted(number): # unit gaussian, mean shifted to 1/sqrt(2)\n",
    "    x = np.random.normal(loc=1/(np.sqrt(2)), scale=1.0, size=(number, 2)) # loc is mean, scale is stdev (not variance)\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "def get_data_gaussian_varied(number, var):\n",
    "    x = np.random.normal(loc=0, scale=np.sqrt(var), size=(number, 2)) # loc is mean, scale is stdev (not variance)\n",
    "    decimated_x = np.sum(x, axis=1) / np.sqrt(2) # array shape (n, 2) -> (n, 1)\n",
    "    return x, decimated_x\n",
    "\n",
    "class gaussian_data(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.samples, self.true_samples = get_data_gaussian(n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "class gaussian_data_vary_std(Dataset):\n",
    "    def __init__(self, n_samples, var):\n",
    "        self.samples, self.true_samples = get_data_gaussian_varied(n_samples, var)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "class gaussian_data_shifted_mean(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.samples, self.true_samples = get_data_gaussian_shifted(n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "class uniform_data(Dataset):\n",
    "    def __init__(self, n_samples):\n",
    "        self.samples, self.true_samples = get_data_uniform(n_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "n_train = int(1e5)\n",
    "n_test = int(1e5)\n",
    "# var = 1\n",
    "\n",
    "train_samples = gaussian_data(n_train)\n",
    "trainloader = DataLoader(train_samples, batch_size=64, shuffle=True)\n",
    "test_samples = uniform_data(n_test)\n",
    "testloader = DataLoader(test_samples, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding statistical source of prediction errors// Identifying reasonable sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrepancy between ground truth and JL expected scaled cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes = [10**3, 5*10**3, 10**4, 5*10**4, 10**5, 5*10**5, 10**6, 5*10**6, 10**7]\n",
    "# n_trials = 5  # number of seeds\n",
    "\n",
    "\n",
    "# # var = 1\n",
    "# errors_kappa4 = []\n",
    "# errors_kappa3 = []\n",
    "# errors_kappa2 = []\n",
    "\n",
    "# trial_data = []\n",
    "\n",
    "# for N in sample_sizes:\n",
    "#     deviations_k4 = []\n",
    "#     deviations_k3 = []\n",
    "#     deviations_k2 = []\n",
    "    \n",
    "#     for trial in range(n_trials):\n",
    "\n",
    "#         test_samples = uniform_data(N)\n",
    "#         test_inputs, _ = find_input_cumulants(test_samples)\n",
    "#         JL_pred_cumulants, ground_truth_cumulants = find_pred_output_cumulants(test_samples)\n",
    "\n",
    "#         # Compare — e.g., deviation of κ₄\n",
    "#         k4_pred = JL_pred_cumulants['kappa4']\n",
    "#         k4_gt = ground_truth_cumulants['kappa4']\n",
    "\n",
    "#         k3_pred = JL_pred_cumulants['kappa3']\n",
    "#         k3_gt = ground_truth_cumulants['kappa3']\n",
    "\n",
    "#         k2_pred = JL_pred_cumulants['kappa2']\n",
    "#         k2_gt = ground_truth_cumulants['kappa2']\n",
    "\n",
    "#         # Compute deviations\n",
    "#         deviation_k4 = np.abs(k4_pred - k4_gt)\n",
    "#         deviation_k3 = np.abs(k3_pred - k3_gt)\n",
    "#         deviation_k2 = np.abs(k2_pred - k2_gt)\n",
    "\n",
    "#         deviations_k4.append(deviation_k4)\n",
    "#         deviations_k3.append(deviation_k3)\n",
    "#         deviations_k2.append(deviation_k2)\n",
    "\n",
    "#         # Save this trial's data\n",
    "#         trial_data.append([N, trial + 1, deviation_k2, deviation_k3, deviation_k4])\n",
    "\n",
    "#     # Store average deviation for this N\n",
    "#     errors_kappa4.append(np.mean(deviations_k4))\n",
    "#     errors_kappa3.append(np.mean(deviations_k3))\n",
    "#     errors_kappa2.append(np.mean(deviations_k2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(sample_sizes, errors_kappa2, marker='o', label='empirical deviations - k2')\n",
    "# plt.plot(sample_sizes, errors_kappa3, marker='s', label='empirical deviations - k3')\n",
    "# plt.plot(sample_sizes, errors_kappa4, marker='x', label='empirical deviations - k4')\n",
    "# expected = [np.sqrt(24 / N) for N in sample_sizes]\n",
    "# plt.plot(sample_sizes, expected, '--', label=\"√(24/N)\")\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Sample size (N)\")\n",
    "# plt.ylabel(\"Mean deviations\")\n",
    "# plt.title(\"Discrepancy vs. Sample Size\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# slope, intercept = np.polyfit(np.log10(sample_sizes), np.log10(errors_kappa4), 1)\n",
    "# print(\"Estimated convergence rate:\", slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Combine sample sizes and corresponding errors into rows\n",
    "# rows = zip(sample_sizes, errors_kappa4)\n",
    "\n",
    "# # Specify output file name\n",
    "# output_file = \"kappa4_samplesize_gaussian.csv\"\n",
    "\n",
    "# # Write to CSV with UTF-8 encoding\n",
    "# with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Sample Size (N)\", \"Mean Deviation in κ4\"])  # Header\n",
    "#     for row in rows:\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"Saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Save summary (mean deviations)\n",
    "# # =============================\n",
    "# summary_file = \"cumulant_deviations_summary.csv\"\n",
    "\n",
    "# with open(summary_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\n",
    "#         \"Sample Size (N)\",\n",
    "#         \"Mean Deviation in κ2 (Variance)\",\n",
    "#         \"Mean Deviation in κ3 (Skewness)\",\n",
    "#         \"Mean Deviation in κ4 (Kurtosis)\"\n",
    "#     ])\n",
    "#     for N, d2, d3, d4 in zip(sample_sizes, errors_kappa2, errors_kappa3, errors_kappa4):\n",
    "#         writer.writerow([N, d2, d3, d4])\n",
    "\n",
    "# print(f\"Saved summary to: {summary_file}\")\n",
    "\n",
    "\n",
    "# # =============================\n",
    "# # Save detailed trial data\n",
    "# # =============================\n",
    "# detailed_file = \"cumulant_deviations_trials.csv\"\n",
    "\n",
    "# with open(detailed_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\n",
    "#         \"Sample Size (N)\",\n",
    "#         \"Trial Number\",\n",
    "#         \"Deviation in κ2\",\n",
    "#         \"Deviation in κ3\",\n",
    "#         \"Deviation in κ4\"\n",
    "#     ])\n",
    "#     writer.writerows(trial_data)\n",
    "\n",
    "# print(f\"Saved detailed trial data to: {detailed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Diagonal L2 Norm Decay for higher order cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes = [10**3, 5*10**3, 10**4, 5*10**4, 10**5, 5*10**5, 10**6, 5*10**6, 10**7]\n",
    "# n_trials = 5  # number of seeds\n",
    "\n",
    "# # Per-sample-size trial data\n",
    "# trial_data_k2 = []\n",
    "# trial_data_k3 = []\n",
    "# trial_data_k4 = []\n",
    "\n",
    "# # Per-sample-size mean (for main trend line)\n",
    "# mean_offdiag_k2 = []\n",
    "# mean_offdiag_k3 = []\n",
    "# mean_offdiag_k4 = []\n",
    "\n",
    "# for N in sample_sizes:\n",
    "#     deviations_k2, deviations_k3, deviations_k4 = [], [], []\n",
    "\n",
    "#     for _ in range(n_trials):\n",
    "#         test_samples = uniform_data(N)\n",
    "#         cumulants = cumulant_extraction_multidim(test_samples.samples)\n",
    "\n",
    "#         # κ2: remove diagonal\n",
    "#         k2 = cumulants['kappa2'].numpy()\n",
    "#         off_k2 = k2 - np.diag(np.diag(k2))\n",
    "#         deviations_k2.append(np.sum(off_k2**2))\n",
    "\n",
    "#         # κ3: mask fully diagonal terms (i=j=k)\n",
    "#         k3 = cumulants['kappa3'].numpy()\n",
    "#         mask3 = np.ones_like(k3, dtype=bool)\n",
    "#         for i in range(k3.shape[0]):\n",
    "#             mask3[i, i, i] = False\n",
    "#         deviations_k3.append(np.sum(k3[mask3]**2))\n",
    "\n",
    "#         # κ4: mask fully diagonal terms (i=j=k=l)\n",
    "#         k4 = cumulants['kappa4'].numpy()\n",
    "#         mask4 = np.ones_like(k4, dtype=bool)\n",
    "#         for i in range(k4.shape[0]):\n",
    "#             mask4[i, i, i, i] = False\n",
    "#         deviations_k4.append(np.sum(k4[mask4]**2))\n",
    "\n",
    "#     # Save per-trial data\n",
    "#     trial_data_k2.append(deviations_k2)\n",
    "#     trial_data_k3.append(deviations_k3)\n",
    "#     trial_data_k4.append(deviations_k4)\n",
    "\n",
    "#     # Save averages\n",
    "#     mean_offdiag_k2.append(np.mean(deviations_k2))\n",
    "#     mean_offdiag_k3.append(np.mean(deviations_k3))\n",
    "#     mean_offdiag_k4.append(np.mean(deviations_k4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Plot average L2 norms for κ₂, κ₃, and κ₄\n",
    "# plt.plot(sample_sizes, mean_offdiag_k2, 'o-', label=r'$\\kappa_2$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k3, 's-', label=r'$\\kappa_3$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k4, '^-', label=r'$\\kappa_4$')\n",
    "\n",
    "# # Log scale\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # Labels and title\n",
    "# plt.xlabel('Sample size (N)')\n",
    "# plt.ylabel('Mean off-diagonal L2 norm')\n",
    "# plt.title('Off-diagonal Discrepancy vs. Sample Size')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# output_file = \"offdiag_deviations_L2norm.csv\"\n",
    "\n",
    "# with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "    \n",
    "#     # Write header\n",
    "#     header = ['Sample Size', 'Cumulant']\n",
    "#     header += [f'Trial {i+1}' for i in range(n_trials)]\n",
    "#     header += ['Mean']\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # Helper to write one cumulant's data\n",
    "#     def write_cumulant_rows(sample_sizes, trial_data, mean_data, cumulant_label):\n",
    "#         for i, N in enumerate(sample_sizes):\n",
    "#             row = [N, cumulant_label]\n",
    "#             row.extend(trial_data[i])\n",
    "#             row.append(mean_data[i])\n",
    "#             writer.writerow(row)\n",
    "\n",
    "#     # Write data for each cumulant\n",
    "#     write_cumulant_rows(sample_sizes, trial_data_k2, mean_offdiag_k2, 'kappa2')\n",
    "#     write_cumulant_rows(sample_sizes, trial_data_k3, mean_offdiag_k3, 'kappa3')\n",
    "#     write_cumulant_rows(sample_sizes, trial_data_k4, mean_offdiag_k4, 'kappa4')\n",
    "\n",
    "# print(f\"Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off diag L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Per-sample-size trial data\n",
    "# trial_data_k2_L1norm = []\n",
    "# trial_data_k3_L1norm = []\n",
    "# trial_data_k4_L1norm = []\n",
    "\n",
    "# # Per-sample-size mean (for main trend line)\n",
    "# mean_offdiag_k2_L1norm = []\n",
    "# mean_offdiag_k3_L1norm = []\n",
    "# mean_offdiag_k4_L1norm = []\n",
    "\n",
    "# for N in sample_sizes:\n",
    "#     deviations_k2, deviations_k3, deviations_k4 = [], [], []\n",
    "\n",
    "#     for _ in range(n_trials):\n",
    "#         test_samples = uniform_data(N)\n",
    "#         cumulants = cumulant_extraction_multidim(test_samples.samples)\n",
    "\n",
    "#         # κ2: remove diagonal\n",
    "#         k2 = cumulants['kappa2'].numpy()\n",
    "#         off_k2 = k2 - np.diag(np.diag(k2))\n",
    "#         deviations_k2.append(np.sum(np.abs(off_k2)))  # L1 norm\n",
    "\n",
    "#         # κ3: mask fully diagonal terms (i=j=k)\n",
    "#         k3 = cumulants['kappa3'].numpy()\n",
    "#         mask3 = np.ones_like(k3, dtype=bool)\n",
    "#         for i in range(k3.shape[0]):\n",
    "#             mask3[i, i, i] = False\n",
    "#         deviations_k3.append(np.sum(np.abs(k3[mask3])))  # L1 norm\n",
    "\n",
    "#         # κ4: mask fully diagonal terms (i=j=k=l)\n",
    "#         k4 = cumulants['kappa4'].numpy()\n",
    "#         mask4 = np.ones_like(k4, dtype=bool)\n",
    "#         for i in range(k4.shape[0]):\n",
    "#             mask4[i, i, i, i] = False\n",
    "#         deviations_k4.append(np.sum(np.abs(k4[mask4])))  # L1 norm\n",
    "\n",
    "#     # Save per-trial data\n",
    "#     trial_data_k2_L1norm.append(deviations_k2)\n",
    "#     trial_data_k3_L1norm.append(deviations_k3)\n",
    "#     trial_data_k4_L1norm.append(deviations_k4)\n",
    "\n",
    "#     # Save averages\n",
    "#     mean_offdiag_k2_L1norm.append(np.mean(deviations_k2))\n",
    "#     mean_offdiag_k3_L1norm.append(np.mean(deviations_k3))\n",
    "#     mean_offdiag_k4_L1norm.append(np.mean(deviations_k4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Plot average L1 norms for κ₂, κ₃, and κ₄\n",
    "# plt.plot(sample_sizes, mean_offdiag_k2_L1norm, 'o-', label=r'$\\kappa_2$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k3_L1norm, 's-', label=r'$\\kappa_3$')\n",
    "# plt.plot(sample_sizes, mean_offdiag_k4_L1norm, '^-', label=r'$\\kappa_4$')\n",
    "\n",
    "# # Log scale\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # Labels and title\n",
    "# plt.xlabel('Sample size (N)')\n",
    "# plt.ylabel('Mean off-diagonal L1 norm')  # <-- updated from L2 to L1\n",
    "# plt.title('Off-diagonal Discrepancy vs. Sample Size (L1 Norm)')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define CSV filename\n",
    "# output_file = \"offdiag_cumulant_L1norms_uniform_task.csv\"\n",
    "\n",
    "# # Open and write to CSV\n",
    "# with open(output_file, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "    \n",
    "#     # Header row\n",
    "#     n_trials = len(trial_data_k2_L1norm[0])\n",
    "#     header = ['SampleSize', 'K2_mean', 'K3_mean', 'K4_mean']\n",
    "#     header += [f'K2_trial_{i+1}' for i in range(n_trials)]\n",
    "#     header += [f'K3_trial_{i+1}' for i in range(n_trials)]\n",
    "#     header += [f'K4_trial_{i+1}' for i in range(n_trials)]\n",
    "#     writer.writerow(header)\n",
    "\n",
    "#     # Write data per sample size\n",
    "#     for idx, N in enumerate(sample_sizes):\n",
    "#         row = [N,\n",
    "#                mean_offdiag_k2_L1norm[idx],\n",
    "#                mean_offdiag_k3_L1norm[idx],\n",
    "#                mean_offdiag_k4_L1norm[idx]]\n",
    "#         row += trial_data_k2_L1norm[idx]\n",
    "#         row += trial_data_k3_L1norm[idx]\n",
    "#         row += trial_data_k4_L1norm[idx]\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"✅ Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define quadratic activation function\n",
    "alpha = 0.5\n",
    "class QuadraticActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return z + alpha*(z ** 2)\n",
    "    \n",
    "# Define leakyReLU slope\n",
    "slope = 0.5\n",
    "\n",
    "# Define model, L=1, N=10, sigma = 0.75\n",
    "class oneDNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \" 1 hidden layer with a quadratic nonlinearity (so can reuse Fischer analysis directly):\"\n",
    "        self.non_linear_activ = nn.Sequential(\n",
    "            nn.Linear(2, 2, bias=False),\n",
    "            # QuadraticActivation(),\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            nn.LeakyReLU(negative_slope=slope),\n",
    "  \n",
    "            nn.Linear(2, 1, bias=False)\n",
    "\n",
    "            # nn.Linear(50, 50, bias=True),\n",
    "            # QuadraticActivation(),\n",
    "            # nn.Linear(50, 50, bias=True),\n",
    "            # QuadraticActivation(),\n",
    "            # nn.Linear(50, 50, bias=True),\n",
    "            # QuadraticActivation(),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.non_linear_activ(x)\n",
    "        return logits\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for i, m in enumerate(self.non_linear_activ):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if i == 0:\n",
    "                    nn.init.normal_(m.weight, mean=0, std=np.sqrt(0.75/2))\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight, mean=0, std=np.sqrt(0.75/2))\n",
    "                # nn.init.normal_(m.bias, mean=0, std=np.sqrt(0.75))\n",
    "                # nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Model // Cumulant Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_affine_gaussian(mean, covariance, W, b=None):\n",
    "    \"\"\" Adjusted for this task\n",
    "     Propagate the first two cumulants through an affine transformation.\"\"\"\n",
    "    mean = mean.to(torch.float64)\n",
    "    covariance = covariance.to(torch.float64)\n",
    "    W = W.to(torch.float64)\n",
    "\n",
    "#    print(f\"\\n-----------\\nStarting propagation across affine layer\")\n",
    "#     print(f\"Test - printing the shapes of the inputs:\")\n",
    "#     print(f\" Mean shape: {mean.shape}\")\n",
    "#     print(f\" Covariance shape: {covariance.shape}\")\n",
    "#     print(f\" W shape: {W.shape}\") \n",
    "    # if b is not None:\n",
    "    #     print(f\" b shape: {b.shape}\")\n",
    "    \n",
    "    # if mean.ndim == 1:  # Ensure it's a column vector for multiplication\n",
    "    #     mean = mean.unsqueeze(1)  # Shape: [n, 1]\n",
    "\n",
    "\n",
    "    # if mean.shape[0] != W.shape[1]:\n",
    "    #     raise ValueError(f\"Size mismatch: Mean size {mean.shape} is incompatible with weight matrix size {W.shape}.\")\n",
    "\n",
    "    if b is not None:\n",
    "        b = b.to(torch.float64)\n",
    "        if b.ndim == 1:\n",
    "            b = b.unsqueeze(1)\n",
    "        mu_Z = (W @ mean).squeeze() + b.squeeze()\n",
    "    else:\n",
    "        mu_Z = (W @ mean).squeeze()\n",
    "    #     mu_Z = W @ mean.T + b\n",
    "    # else:\n",
    "    #     mu_Z = W @ mean.T\n",
    "    cov_Z = W @ covariance @ W.T\n",
    "\n",
    "    # print(f\"\\n Output mean shape: {mu_Z.shape}\")\n",
    "    # print(f\" Output covariance shape: {cov_Z.shape}\")\n",
    "    # print(f\"Propagated across affine layer complete \\n--------------------------\\n\")\n",
    "\n",
    "    return mu_Z, cov_Z\n",
    "\n",
    "def propagate_affine_higher(g3, g4, W):\n",
    "    \"\"\" Adjusted for this task\n",
    "     Propagate the first two cumulants through an affine transformation.\"\"\"\n",
    "    g3 = g3.to(torch.float64)\n",
    "    g4 = g4.to(torch.float64)\n",
    "    W = W.to(torch.float64)\n",
    "\n",
    "    # print(f\"\\n-----------\\nStarting propagation across affine layer\")\n",
    "    # print(f\"Test - printing the shapes of the inputs:\")\n",
    "    # print(f\" Mean shape: {g3.shape}\")\n",
    "    # print(f\" Covariance shape: {g4.shape}\")\n",
    "    # print(f\" W shape: {W.shape}\")\n",
    "    \n",
    "    # Propagate 3rd order cumulant\n",
    "    g3_z = torch.einsum('ri,sj,tk,ijk->rst', W, W, W, g3)\n",
    "\n",
    "    # Propagate 4th order cumulant\n",
    "    g4_z = torch.einsum('ri,sj,tk,ul,ijkl->rstu', W, W, W, W, g4)\n",
    "\n",
    "    # print(f\"Output cumulant shapes:\")\n",
    "    # print(f\"  g3_z shape: {g3_z.shape}\")\n",
    "    # print(f\"  g4_z shape: {g4_z.shape}\")\n",
    "    # print(f\"Propagation across affine layer complete\\n--------------------------\\n\")\n",
    "\n",
    "    return g3_z, g4_z\n",
    "\n",
    "def propagate_quadratic_gaussian(mean, covariance):\n",
    "    \"\"\" Propagate cumulants through a quadratic activation function\n",
    "    Assume the Fischer Gaussian approx, ie this is for wide hidden layers - so only first two cumulants get propagated \"\"\"\n",
    "\n",
    "    # # assuming cumulants higher than second order vanish - which they do for this input distribution\n",
    "    # diagonal_cov = torch.diagonal(covariance, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "    # mu_y = mean + alpha * (mean ** 2 + diagonal_cov)\n",
    "\n",
    "    # cov_y = covariance + 2 * alpha * covariance * (mean.unsqueeze(-1) + mean.unsqueeze(-2)) \\\n",
    "    #     + 2 * alpha**2 * (covariance @ covariance) \\\n",
    "    #     + 4 * alpha**2 * mean.unsqueeze(-1) * covariance * mean.unsqueeze(-2)\n",
    "\n",
    "    # return mu_y, cov_y    \n",
    "\n",
    "    diagonal_cov = torch.diagonal(covariance, dim1=0, dim2=1)  # Shape: [n]\n",
    "    \n",
    "    # Mean propagation (remains a 1D vector of size [n])\n",
    "    mu_y = mean + alpha * (mean ** 2 + diagonal_cov)\n",
    "    \n",
    "    # Covariance propagation (remains a 2D matrix of size [n, n])\n",
    "    mean_outer = mean.unsqueeze(1) * mean.unsqueeze(0)  # Shape: [n, n]\n",
    "\n",
    "    cov_y = covariance + 2 * alpha * (covariance * (mean.unsqueeze(0) + mean.unsqueeze(1))) \\\n",
    "            + 2 * alpha**2 * (covariance @ covariance) \\\n",
    "            + 4 * alpha**2 * mean_outer * covariance\n",
    "\n",
    "    return mu_y, cov_y\n",
    "\n",
    "def propagate_full_gaussian_only(cumulants, weights, biases=None):\n",
    "    \"\"\" Propagates input cumulants through the network (that uses quadratic activation)\n",
    "    Capable of handling varying numbers of layers (we just extract the weights and biases from the model).\n",
    "    \n",
    "     This function only propagates the first two cumulants - ie we truncate, effectively assume all higher ones = 0 \"\"\"\n",
    "\n",
    "    # mu_list = []\n",
    "    # cov_list = []\n",
    "    # for i in range(len(mean_values)):\n",
    "    #     mu = mean_values[i].to(torch.float64)\n",
    "    #     cov = covariances[i].to(torch.float64)\n",
    "\n",
    "    # Extract cumulants\n",
    "    mu = cumulants['kappa1']\n",
    "    cov = cumulants['kappa2']\n",
    "    # G3 = cumulants['kappa3']\n",
    "    # G4 = cumulants['kappa4']\n",
    "\n",
    "\n",
    "    for layer_idx in range(len(weights)):\n",
    "        W = weights[layer_idx].to(torch.float64)\n",
    "        if biases is not None:\n",
    "            bi =  biases[layer_idx].to(torch.float64)\n",
    "\n",
    "        mu = mu.reshape(-1) # Flatten mu to a row vector - this is a quirk of pytorch matrix multiplication\n",
    "        if biases is not None:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W, bi)\n",
    "            # print(f\" Propagated across affine, WITH BIASES!: \\n mu: {mu} \\n cov: {cov}\")\n",
    "        else:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W)\n",
    "            # print(f\"Propagated across affine: \\n mu: {mu} \\n cov: {cov}\")\n",
    "        if layer_idx < len(weights) - 1:\n",
    "            mu, cov = propagate_quadratic_gaussian(mu, cov)\n",
    "            # print(f\"Propagated across quadratic: \\n mu: {mu} \\n cov: {cov}\")\n",
    "    \n",
    "    return mu, cov\n",
    "\n",
    "def propagate_full_linear(cumulants, weights, biases=None):\n",
    "    \"\"\"Propagates input cumulants through a purely linear network.\n",
    "    No nonlinear activation functions are used.\n",
    "    Gaussian approximation is irrelevant here as we aren't using nonlinearities.\"\"\"\n",
    "    \n",
    "    mu, cov, g3, g4 = cumulants['kappa1'], cumulants['kappa2'], cumulants['kappa3'], cumulants['kappa4']\n",
    "\n",
    "    for layer_idx in range(len(weights)):\n",
    "        W = weights[layer_idx].to(torch.float64)\n",
    "        if biases is not None:\n",
    "            b =  biases[layer_idx].to(torch.float64)\n",
    "\n",
    "        mu = mu.reshape(-1) # Flatten mu to a row vector - this is a quirk of pytorch matrix multiplication\n",
    "        if biases is not None:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W, b)\n",
    "            # print(f\" Propagated across affine: \\n mu: {mu} \\n cov: {cov}\")\n",
    "            g3, g4 = propagate_affine_higher(g3, g4, W)\n",
    "            # print(f\" Propagated across affine: \\n g3: {g3} \\n g4: {g4}\")\n",
    "        else:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W)\n",
    "            # print(f\"Propagated across affine: \\n mu: {mu} \\n cov: {cov}\")\n",
    "            g3, g4 = propagate_affine_higher(g3, g4, W)\n",
    "            # print(f\" Propagated across affine: \\n g3: {g3} \\n g4: {g4}\")\n",
    "\n",
    "    return mu, cov, g3, g4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher Orders\n",
    "The below code was an attempt at using higher order cumulants in the input layer, couldn't get it to work though.\n",
    "\n",
    "Separated out the code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_affine_with_higher(mean, covariance, G3, G4, W, b=None):\n",
    "    \"\"\" Adjusted for this task\n",
    "     Propagate the first four cumulants through an affine transformation.\"\"\"\n",
    "    mean = mean.to(torch.float64)\n",
    "    covariance = covariance.to(torch.float64)\n",
    "    W = W.to(torch.float64)\n",
    "    if b is not None:\n",
    "        b = b.to(torch.float64)\n",
    "        mu_Z = W @ mean.T + b.unsqueeze(1)\n",
    "\n",
    "    else:\n",
    "        mu_Z = W @ mean.T\n",
    "    cov_Z = W @ covariance @ W.T\n",
    "\n",
    "    G3_z = np.einsum('ia,jb,kc,abc->ijk', W, W, W, G3)\n",
    "    G4_z = np.einsum('ia,jb,kc,ld,abcd->ijkl', W, W, W, W, G4)\n",
    "\n",
    "    G3_z = torch.tensor(G3_z, dtype=torch.float64)\n",
    "    G4_z = torch.tensor(G4_z, dtype=torch.float64)\n",
    "\n",
    "\n",
    "    return mu_Z.T, cov_Z, G3_z, G4_z\n",
    "\n",
    "def propagate_quadratic_with_higher(mean, covariance, G3, G4):\n",
    "    \"\"\" Propagate cumulants through a quadratic activation function\n",
    "     This is for the input layer - so need to consider how all higher order cumulants \n",
    "        contribute to the mean/cov of postactivations, not just propagating the first two only across the activation \n",
    "        \n",
    "    Mean should be a vector of shape (n,) otherwise the higher order contribution to the cov will throw errors\"\"\"\n",
    "    \n",
    "    # mean\n",
    "    diagonal_cov = torch.diagonal(covariance, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "    mu_y = mean + alpha * (mean ** 2 + diagonal_cov)\n",
    "\n",
    "    # cov\n",
    "    cov_y = covariance + 2 * alpha * covariance * (mean.unsqueeze(-1) + mean.unsqueeze(-2)) \\\n",
    "        + 2 * alpha**2 * (covariance @ covariance) \\\n",
    "        + 4 * alpha**2 * mean.unsqueeze(-1) * covariance * mean.unsqueeze(-2)\n",
    "\n",
    "    # Find the higher order cumulant contribution\n",
    "    n = mean.shape[0] \n",
    "\n",
    "    term1 = alpha * (1 + 2 * alpha * mean.unsqueeze(-1)) * G3  # Shape: (n, n, n)\n",
    "    term1 = term1.sum(dim=2)  # Sum over the last axis to get a matrix of shape (n, n)\n",
    "\n",
    "    term2 = alpha * (1 + 2 * alpha * mean.unsqueeze(-2)) * G3  # Shape: (n, n, n)\n",
    "    term2 = term2.sum(dim=0)  # Sum over the first axis to get a matrix of shape (n, n)\n",
    "\n",
    "    term3 = alpha**2 * G4.sum(dim=(0, 2))  # Sum over axes 1, 3 (ie picking the first i,j to sum over) for a matrix of shape (n, n)\n",
    "\n",
    "    # Combine all terms\n",
    "    higher_order_term = term1 + term2 + term3\n",
    "    cov_y += higher_order_term\n",
    "\n",
    "\n",
    "    return mu_y, cov_y\n",
    "\n",
    "def propagate_full_with_higher(cumulants, weights, biases=None):\n",
    "    \"\"\" Propagates input cumulants through the network (that uses quadratic activation)\n",
    "    Capable of handling varying numbers of layers (we just extract the weights and biases from the model).\n",
    "    \n",
    "     This function considers higher cumulants when propagating across input layer, but \n",
    "        truncates for rest of network (ie assumes higher cumulants vanish) \"\"\"\n",
    "\n",
    "    # Extract cumulants\n",
    "    mu = cumulants['kappa1']\n",
    "    cov = cumulants['kappa2']\n",
    "    G3 = cumulants['kappa3']\n",
    "    G4 = cumulants['kappa4']\n",
    "\n",
    "    # Initial affine layer - propagate all cumulants\n",
    "    W = weights[0].to(torch.float64)\n",
    "    if biases is not None:\n",
    "        b = biases[0].to(torch.float64)\n",
    "    mu = mu.view(-1)  # Flatten mu to a row vector (PyTorch quirk)\n",
    "    \n",
    "    if biases is not None:\n",
    "        mu, cov, G3, G4 = propagate_affine_with_higher(mu, cov, G3, G4, W, b)\n",
    "    else:\n",
    "        mu, cov, G3, G4 = propagate_affine_with_higher(mu, cov, G3, G4, W)\n",
    "\n",
    "    # First nonlinearity - propagate all cumulants with propagate_quadratic_with_higher()\n",
    "    mu, cov = propagate_quadratic_with_higher(mu, cov, G3, G4)\n",
    "\n",
    "    \n",
    "    for layer_idx in range(1, len(weights)): # start from second layer\n",
    "        W = weights[layer_idx].to(torch.float64)\n",
    "        if biases is not None:\n",
    "            b = biases[layer_idx].to(torch.float64)\n",
    "\n",
    "        # Ensure mu is properly shaped before matrix multiplication\n",
    "        if mu.dim() == 1:  # If `mu` is a 1D vector\n",
    "            mu = mu.view(-1)  # Ensure it is flat, (in_features,)\n",
    "        elif mu.dim() == 2 and mu.size(0) == 1:  # If it's a row vector, reshape it\n",
    "            mu = mu.squeeze(0)\n",
    "\n",
    "        if biases is not None:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W, b)\n",
    "        else:\n",
    "            mu, cov = propagate_affine_gaussian(mu, cov, W)\n",
    "        if layer_idx < len(weights) - 1:\n",
    "            mu, cov = propagate_quadratic_gaussian(mu, cov)\n",
    "\n",
    "    \n",
    "    return mu, cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to enforce symmetric weights constraints\n",
    "@torch.no_grad()\n",
    "def enforce_weight_constraints(model):\n",
    "    \"\"\"\n",
    "    Enforce symmetry in the first layer (2x2) and equal weights in the second layer (1x2).\n",
    "    \"\"\"\n",
    "    layers = model.non_linear_activ\n",
    "    layer1 = layers[0]\n",
    "    layer2 = layers[2]\n",
    "\n",
    "    # First layer: symmetric 2x2 weights\n",
    "    if isinstance(layer1, nn.Linear) and layer1.weight.shape == (2, 2):\n",
    "        W = layer1.weight.data\n",
    "\n",
    "        # Extract current entries\n",
    "        a = 0.5 * (W[0, 0] + W[1, 1])  # average of diagonals\n",
    "        b = 0.5 * (W[0, 1] + W[1, 0])  # average of off-diagonals\n",
    "\n",
    "        # Construct constrained matrix\n",
    "        constrained_W = torch.tensor([[a, b],\n",
    "                                      [b, a]], dtype=W.dtype, device=W.device)\n",
    "\n",
    "        layer1.weight.data.copy_(constrained_W)\n",
    "\n",
    "        # Enforce bias vector [b, b]\n",
    "        if layer1.bias is not None:\n",
    "            b = layer1.bias.data\n",
    "            avg_bias = b.mean()\n",
    "            b.fill_(avg_bias)\n",
    "            layer1.bias.data.copy_(b)\n",
    "    \n",
    "    # Second layer: equal weights (1x2)\n",
    "    # Need to change this from 1 to 2 if including nonlinearity\n",
    "    if isinstance(layer2, nn.Linear) and layer2.weight.shape == (1, 2):\n",
    "        W = layer2.weight.data\n",
    "        avg_value = W.mean()\n",
    "        W.fill_(avg_value)\n",
    "        layer2.weight.data.copy_(W)\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_dead_neurons(model, dataloader, threshold=0.99):\n",
    "    \"\"\"\n",
    "    Check for dead neurons in ReLU layers by computing how often their output is zero.\n",
    "\n",
    "    Args:\n",
    "        model: the trained model\n",
    "        dataloader: data loader with input samples\n",
    "        threshold: fraction of zeros above which a neuron is considered dead\n",
    "\n",
    "    Returns:\n",
    "        Dict of dead neuron indices and percentages per layer\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dead_report = {}\n",
    "    \n",
    "    for i, layer in enumerate(model.non_linear_activ):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            continue  # skip linear layers directly\n",
    "        # if isinstance(layer, nn.ReLU) or isinstance(layer, QuadraticActivation):\n",
    "        if isinstance(layer, (nn.ReLU, nn.LeakyReLU, QuadraticActivation)):\n",
    "            # Only apply this for real ReLU-like activations, or Quadratic if you ever extend it\n",
    "            layer_outputs = []\n",
    "\n",
    "            for X, _ in dataloader:\n",
    "                X = X.to(device)\n",
    "                with torch.no_grad():\n",
    "                    # Forward until this layer\n",
    "                    out = X\n",
    "                    for j in range(i + 1):\n",
    "                        out = model.non_linear_activ[j](out)\n",
    "                    layer_outputs.append(out.cpu())\n",
    "\n",
    "            layer_outputs = torch.cat(layer_outputs, dim=0)\n",
    "            # zero_mask = (layer_outputs == 0)\n",
    "\n",
    "            epsilon = 1e-5\n",
    "            zero_mask = (layer_outputs.abs() < epsilon)\n",
    "\n",
    "\n",
    "            zero_fraction = zero_mask.sum(dim=0).float() / zero_mask.shape[0]\n",
    "\n",
    "            dead_neurons = (zero_fraction > threshold).nonzero(as_tuple=True)[0].tolist()\n",
    "            dead_report[f\"Layer {i}\"] = {\n",
    "                \"dead_neurons\": dead_neurons,\n",
    "                \"fractions\": zero_fraction.numpy()\n",
    "            }\n",
    "\n",
    "    return dead_report\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, track_loss=False, track_params=False, symmetry=False):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    loss_steps = []\n",
    "    loss_values = []\n",
    "    param_steps = []\n",
    "    param_changes = []\n",
    "    step = 0\n",
    "\n",
    "    if track_params:\n",
    "        prev_params = {name: param.clone().detach() for  name, param in model.named_parameters()}\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device).unsqueeze(1)\n",
    "\n",
    "        # Compute prediction error for forward pass on current batch\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if symmetry:\n",
    "            # 🔧 Enforce symmetry and equal weights\n",
    "            enforce_weight_constraints(model)\n",
    "\n",
    "        \n",
    "        # Track loss\n",
    "        if track_loss:\n",
    "            loss_steps.append(step)\n",
    "            loss_values.append(loss.item()) # this is already a per-batch average loss\n",
    "        \n",
    "        # Track parameter changes every 10 steps/batches\n",
    "        if track_params and step % 10 ==0 and step > 0:\n",
    "            total_param_change = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                param_diff = param.detach() - prev_params[name]\n",
    "                # total_param_change += torch.norm(param_diff, p=2).item()\n",
    "                total_param_change += torch.sum(param_diff ** 2).item()  # Direct sum of squared differences\n",
    "                prev_params[name] = param.clone().detach()\n",
    "            param_steps.append(step)\n",
    "            param_changes.append(np.sqrt(total_param_change))\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Current batch: {current:>5d}/{size:>5d}\\n  Loss of batch: {loss:>7f}\")\n",
    "\n",
    "        step += 1\n",
    "    \n",
    "    return loss_steps, loss_values, param_steps, param_changes\n",
    "\n",
    "\n",
    "def collect_model_outputs(dataloader, model):\n",
    "    \"\"\"\n",
    "    Performs a single forwards pass, then collects all outputs from the model for the given dataloader and\n",
    "      stacks them into a 1D numpy array.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (DataLoader): The DataLoader to iterate over.\n",
    "        model (torch.nn.Module): The trained model whose outputs are to be collected.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D NumPy array of all model outputs concatenated together.\n",
    "    \"\"\"\n",
    "    model.eval()  # Switch to evaluation mode (no gradients tracked)\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for X, _ in dataloader:\n",
    "            X = X.to(device)\n",
    "            outputs = model(X)\n",
    "            all_outputs.append(outputs.squeeze().cpu().numpy())  # Ensure outputs are 1D and on CPU\n",
    "\n",
    "    # Concatenate all outputs into a single 1D NumPy array of size (N,)\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "\n",
    "def plot_histogram(data, title=None, bins=50, color='blue', show=False, norm=False, label=None, legend=None, edgecolor='black'):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data to plot.\n",
    "        title (str): Title of the plot.\n",
    "        bins (int): Number of bins in the histogram.\n",
    "        color (str): Color of the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    if norm:\n",
    "        plt.hist(data, bins=bins, color=color, label=label, density=True, alpha=0.7, edgecolor=edgecolor)\n",
    "    else:\n",
    "        plt.hist(data, bins=bins, color=color, alpha=0.7, edgecolor='black')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_histogram_thesis(data, bins=50, color='blue', show=False, \n",
    "                   label=None, legend=None, edgecolor='none', alpha=0.6, leg_loc='best', \n",
    "                   axis_fontsize=12, tick_fontsize=11, legend_fontsize=12, output_file=False):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data to plot.\n",
    "        title (str): Title of the plot.\n",
    "        bins (int): Number of bins in the histogram.\n",
    "        color (str): Color of the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.hist(data, bins=bins, color=color, label=label if label else None, \n",
    "             density=True, alpha=alpha, edgecolor=edgecolor)\n",
    "\n",
    "    plt.xlabel('Network output $y$', fontsize=axis_fontsize)\n",
    "    plt.ylabel('$p(y)$', fontsize=axis_fontsize)\n",
    "    plt.xticks(fontsize=tick_fontsize)\n",
    "    plt.yticks(fontsize=tick_fontsize)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(frameon=False, loc=leg_loc, fontsize=legend_fontsize)\n",
    "    plt.tight_layout()\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network Model\n",
    "\n",
    "Using MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch:    64/100000\n",
      "  Loss of batch: 1.502626\n",
      "Current batch: 32064/100000\n",
      "  Loss of batch: 0.101202\n",
      "Current batch: 64064/100000\n",
      "  Loss of batch: 0.093481\n",
      "Current batch: 96064/100000\n",
      "  Loss of batch: 0.082475\n",
      "Current batch:    64/100000\n",
      "  Loss of batch: 0.106397\n",
      "Current batch: 32064/100000\n",
      "  Loss of batch: 0.070063\n",
      "Current batch: 64064/100000\n",
      "  Loss of batch: 0.081837\n",
      "Current batch: 96064/100000\n",
      "  Loss of batch: 0.089931\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "\n",
      "Name: non_linear_activ.0.weight\n",
      "Shape: torch.Size([2, 2])\n",
      "Values: tensor([[-0.5869, -0.7334],\n",
      "        [-0.7334, -0.5869]])\n",
      "Name: non_linear_activ.2.weight\n",
      "Shape: torch.Size([1, 2])\n",
      "Values: tensor([[-0.6354, -0.6354]])\n"
     ]
    }
   ],
   "source": [
    "model = oneDNetwork().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "\n",
    "# train, plot loss\n",
    "tot_loss = []\n",
    "tot_steps_loss = []\n",
    "# tot_param_changes = []\n",
    "# tot_param_steps = []\n",
    "loss_step_count = 0\n",
    "\n",
    "for epoch in range(2):\n",
    "    loss1_steps, loss1_values, param1_steps, param1_changes = train(trainloader, model, loss_fn, optimizer, track_loss=True, track_params=False, symmetry=True)\n",
    "    # param1_steps = [x + loss_step_count for x in param1_steps]\n",
    "    loss1_steps = [x + loss_step_count for x in loss1_steps]\n",
    "    loss_step_count += len(loss1_steps)\n",
    "\n",
    "    tot_loss.extend(loss1_values)\n",
    "    tot_steps_loss.extend(loss1_steps)\n",
    "    # tot_param_changes.extend(param1_changes)\n",
    "    # tot_param_steps.extend(param1_steps)\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    \"step\": tot_steps_loss,\n",
    "    \"loss\": tot_loss\n",
    "})\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "loss_df.to_csv(f\"training_loss_{timestamp}.csv\", index=False)\n",
    "\n",
    "\n",
    "# ONLY FOR RELU EXPERIMENTS\n",
    "# Check for dead neurons\n",
    "dead_info = check_dead_neurons(model, trainloader)\n",
    "\n",
    "# Extract per-neuron zero output fraction (for the single nonlinear layer)\n",
    "# Assuming only one nonlinear layer, so we just grab its first entry\n",
    "dead_layer_key = list(dead_info.keys())[0]  # e.g., 'Layer 1'\n",
    "zero_fractions = dead_info[dead_layer_key]['fractions']  # This is a NumPy array\n",
    "\n",
    "\n",
    "# Optionally also write the total dead neuron *fraction* (e.g., number dead / total)\n",
    "n_neurons = len(zero_fractions)\n",
    "n_dead = len(dead_info[dead_layer_key][\"dead_neurons\"])\n",
    "total_dead_frac = n_dead / n_neurons if n_neurons > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(tot_steps_loss, tot_loss, label='Loss', color='blue', alpha=0.7)\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Network Loss $\\mathcal{L}$\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print out model parameters to check\n",
    "print(\"Model Parameters:\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param.data}\")\n",
    "\n",
    "# Save the model weights to a file (e.g., 'model_weights.pth')\n",
    "model_weights = {name: param.data for name, param in model.named_parameters()}\n",
    "torch.save(model_weights, 'weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "These are asymmetric solutions - not consistent with my analytical results.\n",
    "\n",
    "Only sometimes does it seem to converge to the expected 'columns same'/'across diagonals' symmetric cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Histogram + Overlay\n",
    "\n",
    "Here we want to compare a histogram of the raw network outputs to an overlaid plot of the propagated theoretical probability density (using Fischer framework).\n",
    "\n",
    "This functions as a check of the Fischer framework validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shuffled_data(Dataset):\n",
    "    def __init__(self, inputs, ground_outputs):\n",
    "        self.samples, self.true_samples = inputs, ground_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.samples[idx], dtype=torch.float64),\n",
    "            torch.tensor(self.true_samples[idx], dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "# Network output cumulants\n",
    "def find_network_output_cumulants(data_loader, model_x):\n",
    "\n",
    "    network_outputs = collect_model_outputs(data_loader, model_x)\n",
    "    network_cumulants = cumulant_extraction(network_outputs)\n",
    "    print(\"\\nExtracted Cumulants (of raw network output):\")\n",
    "    for k, v in network_cumulants.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return network_outputs, network_cumulants\n",
    "\n",
    "def find_netout_cumulants_shuffled(data_loader, model_x, get_iterated=False):\n",
    "\n",
    "    network_outputs = collect_model_outputs(data_loader, model_x)\n",
    "    network_cumulants = cumulant_extraction(network_outputs)\n",
    "    print(\"\\nExtracted Cumulants (of raw network output):\")\n",
    "    for k, v in network_cumulants.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if get_iterated:\n",
    "        shufffled_outputs = np.random.permutation(network_outputs)  # Shuffle the outputs\n",
    "        new_inputs = np.column_stack((network_outputs, shufffled_outputs))  # Shape: (N, 2)\n",
    "        decimated_outputs = np.sum(new_inputs, axis=1) / np.sqrt(2)  # Shape: (N,)\n",
    "        \n",
    "        iterated_data = shuffled_data(new_inputs, decimated_outputs)\n",
    "    \n",
    "    return network_outputs, network_cumulants, iterated_data\n",
    "\n",
    "def extract_weights(model):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer in model.non_linear_activ:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            W = layer.weight.detach().cpu().numpy()\n",
    "            weights.append(torch.tensor(W, dtype=torch.float64))\n",
    "            if layer.bias is not None:\n",
    "                b = layer.bias.detach().cpu().numpy()\n",
    "                biases.append(torch.tensor(b, dtype=torch.float64))\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "def JL_cumulant_evolution(data, num_iterations):\n",
    "    \"\"\" Input: data is a dictionary of cumulants, num_iterations is the number of iterations to perform.\n",
    "        Output: a dictionary of cumulants after num_iterations iterations.\n",
    "        \n",
    "        The functions scales each nth order cumulant by the appropriate factor for the number of iterations.\n",
    "        \n",
    "        Was num_iterations + 1 (previous version of code), have now changed it to just num_iterations.\"\"\"\n",
    "    \n",
    "    scaled_cumulants = {}\n",
    "    for key, value in data.items():\n",
    "        # Extract the cumulant order (k) from the key string 'kappa{k}'\n",
    "        k = int(key[-1])\n",
    "        \n",
    "        # Calculate the scaling factor\n",
    "        scaling_factor = 2 ** (1 - k / 2)\n",
    "        \n",
    "        # Scale the cumulant value num_iterations times\n",
    "        scaled_cumulants[key] = value * (scaling_factor ** (num_iterations))\n",
    "    \n",
    "    return scaled_cumulants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def pdf_from_cumulants(y_vals, cumulants, t_max=50.0, num_points=2**12, plot=False):\n",
    "    \"\"\"\n",
    "    Construct a PDF from a list of cumulants using characteristic function inversion.\n",
    "\n",
    "    Args:\n",
    "        y_vals (np.ndarray): Points at which to evaluate the PDF.\n",
    "        cumulants (list): List of raw cumulants (may include PyTorch tensors).\n",
    "        t_max (float): Max frequency for integration.\n",
    "        num_points (int): Number of FFT points (preferably power of 2).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Approximated PDF values.\n",
    "    \"\"\"\n",
    "    t = np.linspace(-t_max, t_max, num_points)\n",
    "    dt = t[1] - t[0]\n",
    "\n",
    "    log_phi = np.zeros_like(t, dtype=np.complex128)\n",
    "\n",
    "    for n, kappa in enumerate(cumulants, start=1):\n",
    "        if hasattr(kappa, 'item'):  # Convert PyTorch tensor to scalar\n",
    "            kappa = kappa.item()\n",
    "        log_phi += (1j * t)**n * kappa / math.factorial(n)\n",
    "\n",
    "    log_phi = np.clip(np.real(log_phi), -500, 500) + 1j * np.imag(log_phi)\n",
    "    phi = np.exp(log_phi)\n",
    "\n",
    "    # # phase correct\n",
    "    # mean = cumulants[0]\n",
    "    # phi *= np.exp(-1j * mean * t)\n",
    "\n",
    "    # Inverse Fourier transform\n",
    "    freqs = np.fft.fftfreq(num_points, d=dt)\n",
    "    y_grid = np.fft.fftshift(freqs) * 2 * np.pi\n",
    "\n",
    "    # y_grid = np.linspace(-np.pi / dt, np.pi / dt, num_points)\n",
    "    fft_pdf = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(phi))) * dt / (2 * np.pi)\n",
    "\n",
    "    # # Interpolate onto desired y_vals\n",
    "    # interp_pdf = np.interp(y_vals, y_grid, np.real(fft_pdf))\n",
    "\n",
    "    # Smooth interpolation\n",
    "    interpolator = interp1d(y_grid, np.real(fft_pdf), kind='cubic', fill_value='extrapolate')\n",
    "    interp_pdf = interpolator(y_vals)\n",
    "\n",
    "    # Normalize\n",
    "    interp_pdf /= np.trapezoid(interp_pdf, y_vals)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(t, np.abs(phi), label='mod phi')\n",
    "        plt.axvline(t_max, color='r', linestyle='--', label='t_max')\n",
    "        plt.title('Magnitude of characteristic function')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    return interp_pdf\n",
    "\n",
    "def theoretical_pdf_normal(y_vals, mu, cov):\n",
    "    \"\"\"  \n",
    "    Plot Theoretical Probability Density Function using a single 1D mean and variance.\n",
    "    \n",
    "    Args:\n",
    "    - y_vals (np.ndarray): Array of y values where the PDF is evaluated.\n",
    "    - mu (torch.Tensor): Scalar tensor representing the mean.\n",
    "    - cov (torch.Tensor): Scalar tensor representing the variance.\n",
    "    \n",
    "    Returns:\n",
    "    - pdf (np.ndarray): Theoretical probability density function values.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy scalars\n",
    "    mu = mu.item()\n",
    "    variance = cov.item()\n",
    "    sigma = variance ** 0.5  # Convert variance to standard deviation\n",
    "\n",
    "    # Compute the PDF using the normal distribution\n",
    "    pdf = norm.pdf(y_vals, loc=mu, scale=sigma)\n",
    "\n",
    "    # Normalize the PDF so it integrates to 1\n",
    "    pdf /= np.trapezoid(pdf, y_vals)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "# def theoretical_pdf_higher(y_vals, k1, k2, k3, k4):\n",
    "    \"\"\"  \n",
    "    Plot Theoretical PDF using the first four cumulants via Edgeworth expansion.\n",
    "    \n",
    "    Args:\n",
    "    - y_vals (np.ndarray): Points at which to evaluate the PDF.\n",
    "    - k1 (torch.Tensor): First cumulant (mean).\n",
    "    - k2 (torch.Tensor): Second cumulant (variance).\n",
    "    - k3 (torch.Tensor): Third cumulant.\n",
    "    - k4 (torch.Tensor): Fourth cumulant.\n",
    "    \n",
    "    Returns:\n",
    "    - pdf (np.ndarray): Approximated PDF values.\n",
    "    \"\"\"\n",
    "    # Convert to scalars\n",
    "    mu = k1.item()\n",
    "    var = k2.item()\n",
    "    c3 = k3.item()\n",
    "    c4 = k4.item()\n",
    "    \n",
    "    sigma = np.sqrt(var)\n",
    "    z = (y_vals - mu) / sigma\n",
    "    \n",
    "    # Standard normal PDF\n",
    "    phi = norm.pdf(z)\n",
    "    \n",
    "    # Hermite polynomials\n",
    "    H3 = hermite(3)(z)\n",
    "    H4 = hermite(4)(z)\n",
    "\n",
    "    # Edgeworth correction using raw cumulants\n",
    "    skew_term = (c3 / (6 * sigma**3)) * H3\n",
    "    kurt_term = (c4 / (24 * sigma**4)) * H4\n",
    "    edgeworth_corr = 1 + skew_term + kurt_term\n",
    "\n",
    "    pdf = phi * edgeworth_corr / sigma  # De-standardize\n",
    "\n",
    "    # Normalize to integrate to 1\n",
    "    pdf /= np.trapezoid(pdf, y_vals)\n",
    "\n",
    "    pdf = np.clip(pdf, 0, None)\n",
    "\n",
    "    return pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import rel_entr  # element-wise P * log(P / Q)\n",
    "\n",
    "def norm_kl_div_histtopdf(samples, pdf_vals, y_vals):\n",
    "    \"\"\"\n",
    "    Compute normalized KL divergence: D_KL(P_emp || P_theo) / H(P_emp), normalising by the Shannon entropy.\n",
    "\n",
    "    Use y_vals as pdf evaluation points = bin centres, convert to bin edges.\n",
    "    Converts density into discrete probability mass function (PMF) using histogram.\n",
    "    Normalises the PMF to ensure it sums to 1.\n",
    "    Clipping to avoid log(0) and other numerical issues.\n",
    "\n",
    "    KL divergence is outputted in units of nats, but this isn't a physical unit, rather a measure of relative information between distributions.\n",
    "        This normalised KL divergence is unitless, and ranges from 0 to 1.\n",
    "    0 = perfect match, 1 = divergence as large as the entropy. \"\"\"\n",
    "\n",
    "    # Step 1: Histogram\n",
    "    dy = y_vals[1] - y_vals[0]  # spacing between points\n",
    "    bin_edges = np.linspace(y_vals[0] - dy / 2, y_vals[-1] + dy / 2, len(y_vals) + 1) # ensures that distribs are defined over matching bins (diff histogram to the previous plots with bin=100)\n",
    "\n",
    "    hist, bin_edges = np.histogram(samples, bins=bin_edges, density=True)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "\n",
    "    # Empirical PMF\n",
    "    p_empirical = hist * bin_widths\n",
    "\n",
    "    # Interpolate theoretical PDF to match bin centers\n",
    "    pdf_interp = np.interp(bin_centers, y_vals, pdf_vals)\n",
    "    p_theoretical = pdf_interp * bin_widths\n",
    "    p_theoretical /= np.sum(p_theoretical)\n",
    "\n",
    "    # Clip to avoid numerical issues\n",
    "    eps = 1e-12\n",
    "    p_empirical = np.clip(p_empirical, eps, 1)\n",
    "    p_theoretical = np.clip(p_theoretical, eps, 1)\n",
    "\n",
    "    # KL divergence and entropy\n",
    "    kl = np.sum(rel_entr(p_empirical, p_theoretical))\n",
    "    entropy = -np.sum(p_empirical * np.log(p_empirical))\n",
    "\n",
    "    normalized_kl = kl / entropy\n",
    "    return normalized_kl\n",
    "\n",
    "def norm_kl_div_pdf2pdf(p_vals, q_vals, y_vals, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute normalized KL divergence: D_KL(P || Q) / H(P)\n",
    "\n",
    "    Args:\n",
    "        p_vals (np.ndarray): First PDF values (e.g., empirical).\n",
    "        q_vals (np.ndarray): Second PDF values (e.g., theoretical).\n",
    "        y_vals (np.ndarray): Evaluation points (must be evenly spaced).\n",
    "        eps (float): Small value to avoid log(0) or division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized KL divergence (unitless, range 0–1)\n",
    "    \"\"\"\n",
    "    dy = y_vals[1] - y_vals[0]\n",
    "\n",
    "    # Clip PDFs to avoid log(0) or division by 0\n",
    "    p_safe = np.clip(p_vals, eps, 1)\n",
    "    q_safe = np.clip(q_vals, eps, 1)\n",
    "\n",
    "    # KL divergence and entropy (discretized)\n",
    "    kl = np.sum(rel_entr(p_safe, q_safe)) * dy\n",
    "    entropy = -np.sum(p_safe * np.log(p_safe)) * dy\n",
    "\n",
    "    normalized_kl = kl / entropy\n",
    "    return normalized_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def wasserstein_histtopdf(samples, pdf_vals, y_vals):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein distance between empirical samples and a theoretical PDF evaluated on y_vals.\n",
    "\n",
    "    Args:\n",
    "        samples (np.ndarray): Empirical samples.\n",
    "        pdf_vals (np.ndarray): Theoretical PDF values (must match y_vals).\n",
    "        y_vals (np.ndarray): Points at which PDF is evaluated (assumed evenly spaced).\n",
    "\n",
    "    Returns:\n",
    "        float: Wasserstein distance (same units as y_vals)\n",
    "    \"\"\"\n",
    "    dy = y_vals[1] - y_vals[0]\n",
    "\n",
    "    # Create a sample-based approximation of the PDF (discrete measure)\n",
    "    theoretical_mass = pdf_vals * dy\n",
    "    theoretical_mass /= np.sum(theoretical_mass)  # normalize to sum to 1\n",
    "\n",
    "    # Treat y_vals as support and theoretical_mass as weights\n",
    "    return wasserstein_distance(u_values=samples, v_values=y_vals, v_weights=theoretical_mass)\n",
    "\n",
    "def wasserstein_pdf2pdf(p_vals, q_vals, y_vals):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein distance between two PDFs evaluated on a shared grid.\n",
    "\n",
    "    Args:\n",
    "        p_vals (np.ndarray): PDF 1 (e.g., empirical).\n",
    "        q_vals (np.ndarray): PDF 2 (e.g., theoretical).\n",
    "        y_vals (np.ndarray): Evaluation points (assumed evenly spaced).\n",
    "\n",
    "    Returns:\n",
    "        float: Wasserstein distance\n",
    "    \"\"\"\n",
    "    dy = y_vals[1] - y_vals[0]\n",
    "\n",
    "    # Normalize both PDFs to sum to 1\n",
    "    p_mass = p_vals * dy\n",
    "    q_mass = q_vals * dy\n",
    "    p_mass /= np.sum(p_mass)\n",
    "    q_mass /= np.sum(q_mass)\n",
    "\n",
    "    return wasserstein_distance(u_values=y_vals, v_values=y_vals, u_weights=p_mass, v_weights=q_mass)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate Cumulants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 0\n",
      "weights: [tensor([[-0.5869, -0.7334],\n",
      "        [-0.7334, -0.5869]]), tensor([[-0.6354, -0.6354]])]\n",
      "biases: []\n",
      "\n",
      "~~~Here's a check that I've done it right!~~~~:\n",
      "Predicted Cumulants (after all iterations):\n",
      "kappa1: 15.997035470025137\n",
      "kappa2: 0.08344120215877884\n",
      "kappa3: 1.6620320772653857e-07\n",
      "kappa4: -8.166978225739125e-06\n",
      "\n",
      "Extracted Cumulants (of raw network output):\n",
      "kappa1: 0.41937046502112835\n",
      "kappa2: 0.029214635753667475\n",
      "kappa3: 9.884272446466197e-06\n",
      "kappa4: -0.0005045124020006922\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'wb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 223\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Load your workbook and sheet\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# wb = load_workbook(r\"C:\\Users\\cga28\\Documents\\Project\\leaky_relu_uniform.xlsx\")\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[43mwb\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.5 LR symm\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# or the exact name of your sheet\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Example: write raw cumulants starting at column H (8), row 3\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# start_row = 12\u001b[39;00m\n\u001b[0;32m    227\u001b[0m start_row \u001b[38;5;241m=\u001b[39m find_next_empty_row(ws)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wb' is not defined"
     ]
    }
   ],
   "source": [
    "# First extract weights and biases from the model\n",
    "weights, biases = extract_weights(model)\n",
    "print(f\"{len(weights)}, {len(biases)}\")\n",
    "print(f\"weights: {weights}\")\n",
    "print(f\"biases: {biases}\")\n",
    "\n",
    "input_samples, input_loader = test_samples, testloader\n",
    "JL_test_samples = test_samples\n",
    "num_iterations = 10\n",
    "\n",
    "# As a check compare the expected JL evolution of cumulants after num_iterations\n",
    "avg_initial_cumulants, _ = find_input_cumulants(JL_test_samples)\n",
    "predicted_cumulants = JL_cumulant_evolution(avg_initial_cumulants, num_iterations)\n",
    "print(\"\\n~~~Here's a check that I've done it right!~~~~:\\nPredicted Cumulants (after all iterations):\")\n",
    "for k, v in predicted_cumulants.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "for number in range(num_iterations):\n",
    "\n",
    "    # region\n",
    "\n",
    "    #############\n",
    "    # Find and save the JL prediction cumulants after this num_iterations\n",
    "    #############\n",
    "    JL_number = number + 1\n",
    "    JL_expected_c = JL_cumulant_evolution(avg_initial_cumulants, JL_number)\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region\n",
    "    #############\n",
    "    # Cumulant extraction, get next dataset\n",
    "    #############\n",
    "\n",
    "    # print(f\"\\nExpected cumulants for uniform distribution: \\n {1/2}, {1/12}, {0}, {-1/120}\")  # Expected cumulants for uniform distribution\n",
    "    avg_input_c, z_dict = find_input_cumulants(input_samples)\n",
    "    avg_jl_cumulants, ground_truth_c = find_pred_output_cumulants(input_samples)\n",
    "    network_outputs, network_cumulants, next_iteration_data_samples = find_netout_cumulants_shuffled(input_loader, model, get_iterated=True)\n",
    "\n",
    "    # Redefine dataset for next iteration\n",
    "    input_samples = next_iteration_data_samples\n",
    "    input_loader = DataLoader(input_samples, batch_size=64, shuffle=False)\n",
    "    \n",
    "\n",
    "\n",
    "    # # Propagate cumulants\n",
    "\n",
    "    # # Networks with nonlinearity\n",
    "    # mu_out, cov_out = propagate_full_gaussian_only(z_dict, weights, biases=None) # if using biases, set biases = True\n",
    "    # cumulants_plotting = [mu_out, cov_out]\n",
    "\n",
    "    # # # Networks without nonlinearity\n",
    "    # # mu_out, cov_out, g3_out, g4_out = propagate_full_linear(z_dict, weights, biases=None) # if using biases, set biases = True\n",
    "    # # cumulants_plotting = [mu_out, cov_out, g3_out, g4_out]\n",
    "\n",
    "    # print(\"\\n\\n------------\\nPropagation complete. Propagated cumulants:\")\n",
    "    # print(f\" mean: {mu_out} \\n cov: {cov_out}\\n\")\n",
    "    # # print(f\" g3: {g3_out} \\n g4: {g4_out}\\n\") # include if using higher cumulants\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # endregion\n",
    "    \n",
    "    # region\n",
    "    ###############\n",
    "    # Plot this iteration\n",
    "    ###############\n",
    "\n",
    "    y_vals = np.linspace(np.min(network_outputs), np.max(network_outputs), 500)\n",
    "\n",
    "    # # Find the pdf using propagated cumulants\n",
    "    # pdf_truncated_propagated = theoretical_pdf_normal(y_vals, mu_out, cov_out)\n",
    "    # pdf_higher_propagated = pdf_from_cumulants(y_vals, cumulants_plotting, t_max=80, num_points=2**13, plot=True)\n",
    "\n",
    "    plot_histogram(network_outputs, bins=100, color='orange', show=True, norm=True, edgecolor='none')\n",
    "\n",
    "    plot_histogram_thesis(network_outputs, bins=100, show=True, alpha=0.4)\n",
    "\n",
    "    # # Plot with both overlaid\n",
    "    # plot_histogram(network_outputs, bins=100, color='orange', show=False, norm=True)\n",
    "    # plt.plot(y_vals, pdf_truncated_propagated, label=\"Gaussian\", color='black', linewidth=2)\n",
    "    # plt.plot(y_vals, pdf_higher_propagated, label=\"With $\\kappa_{3}, \\kappa_{4}$\", color='blue', linewidth=2)\n",
    "\n",
    "    # # plt.title(\"Empirical Data vs Theoretical PDF\")\n",
    "    # plt.xlabel(\"Network Output $y$\")\n",
    "    # plt.ylabel(\"Probability Density $p(y)$\")\n",
    "    # plt.legend(fontsize=9)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # # Plot with just truncated\n",
    "    # plot_histogram(network_outputs, bins=100, color='orange', show=False, norm=True)\n",
    "    # plt.plot(y_vals, pdf_truncated_propagated, label=\"Gaussian\", color='black', linewidth=2)\n",
    "    # plt.xlabel(\"Network Output $y$\")\n",
    "    # plt.ylabel(\"Probability Density $p(y)$\")\n",
    "    # plt.legend(fontsize=9)\n",
    "    # plt.show()\n",
    "\n",
    "    # # Plot with just higher\n",
    "    # plot_histogram(network_outputs, bins=100, color='orange', show=False, norm=True)\n",
    "    # plt.plot(y_vals, pdf_higher_propagated, label=\"With $\\kappa_{3}, \\kappa_{4}$\", color='blue', linewidth=2)\n",
    "    # # plt.axvline(x=cumulants_plotting[0], color='red', linestyle='--', label='Mean')\n",
    "    # plt.xlabel(\"Network Output $y$\")\n",
    "    # plt.ylabel(\"Probability Density $p(y)$\")\n",
    "    # plt.legend(fontsize=9)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region\n",
    "    ##################\n",
    "    # Consistency with analytical results\n",
    "    ##################\n",
    "\n",
    "    # layer1, layer2 = weights[0], weights[1]\n",
    "    # w0, w1, w2, w3 = layer1[0, 0], layer1[0, 1], layer1[1, 0], layer1[1, 1]\n",
    "    # a0, a1 = layer2[0, 0], layer2[0, 1]\n",
    "\n",
    "    # if biases:\n",
    "    #     biases1, biases2 = biases[0], biases[1]\n",
    "    #     b0, b1 = biases1[0], biases1[1]\n",
    "    #     b2 = biases2[0]\n",
    "\n",
    "    # w0_save = w0.item()\n",
    "    # s2_2a0_w1 = np.sqrt(2)/(2 * a0) - w1\n",
    "    # s2_2a0_w1 = s2_2a0_w1.item()\n",
    "\n",
    "    # print(f\"w0: {w0_save}, sqrt(2)/2a0 - w1: {s2_2a0_w1}\")\n",
    "    # if biases:\n",
    "    #     print(f\"b2: {b2}, -2*a0*b1: {-2 * a0 * b1}\")\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region\n",
    "    ###################\n",
    "    # KL divergence\n",
    "    ###################\n",
    "\n",
    "    # Ground truth\n",
    "    gt_cumulants_list_linear = [ground_truth_c['kappa1'], ground_truth_c['kappa2'], ground_truth_c['kappa3'], ground_truth_c['kappa4']]\n",
    "    gt_pdf_trunc = theoretical_pdf_normal(y_vals, ground_truth_c['kappa1'], ground_truth_c['kappa2'])\n",
    "    gt_pdf_higher = pdf_from_cumulants(y_vals, gt_cumulants_list_linear, t_max=80, num_points=2**13)\n",
    "\n",
    "    # JL predictions\n",
    "    jl_cumulants_list_linear = [avg_jl_cumulants['kappa1'], avg_jl_cumulants['kappa2'], avg_jl_cumulants['kappa3'], avg_jl_cumulants['kappa4']]\n",
    "    jl_pdf_trunc = theoretical_pdf_normal(y_vals, avg_jl_cumulants['kappa1'], avg_jl_cumulants['kappa2'])\n",
    "    jl_pdf_higher = pdf_from_cumulants(y_vals, jl_cumulants_list_linear, t_max=80, num_points=2**13)\n",
    "\n",
    "\n",
    "    # Find KL divergences\n",
    "\n",
    "    # network output/ground truth\n",
    "    kl_netout_gt_trunc = norm_kl_div_histtopdf(network_outputs, gt_pdf_trunc, y_vals)\n",
    "    kl_netout_gt_higher = norm_kl_div_histtopdf(network_outputs, gt_pdf_higher, y_vals)\n",
    "\n",
    "    # # propagated/ground truth\n",
    "    # kl_propagated_gt_trunc = norm_kl_div_pdf2pdf(pdf_truncated_propagated, gt_pdf_trunc, y_vals)\n",
    "    # kl_propagated_gt_higher = norm_kl_div_pdf2pdf(pdf_higher_propagated, gt_pdf_higher, y_vals)\n",
    "\n",
    "    # # network output/propagated\n",
    "    # kl_netout_propagated_trunc = norm_kl_div_histtopdf(network_outputs, pdf_truncated_propagated, y_vals)\n",
    "    # kl_netout_propagated_higher = norm_kl_div_histtopdf(network_outputs, pdf_higher_propagated, y_vals)\n",
    "\n",
    "    # JL predictions/ground truth\n",
    "    kl_jl_gt_trunc = norm_kl_div_pdf2pdf(jl_pdf_trunc, gt_pdf_trunc, y_vals)\n",
    "    kl_jl_gt_higher = norm_kl_div_pdf2pdf(jl_pdf_higher, gt_pdf_higher, y_vals)\n",
    "\n",
    "    # network output/JL predictions\n",
    "    kl_netout_jl_trunc = norm_kl_div_histtopdf(network_outputs, jl_pdf_trunc, y_vals)\n",
    "    kl_netout_jl_higher = norm_kl_div_histtopdf(network_outputs, jl_pdf_higher, y_vals)\n",
    "\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region\n",
    "    ##################\n",
    "    # Wasserstein distance\n",
    "    ##################\n",
    "\n",
    "    # Find the distances\n",
    "\n",
    "    # network output/ground truth\n",
    "    wd_netout_gt_trunc = norm_kl_div_histtopdf(network_outputs, gt_pdf_trunc, y_vals)\n",
    "    wd_netout_gt_higher = norm_kl_div_histtopdf(network_outputs, gt_pdf_higher, y_vals)\n",
    "\n",
    "    # # propagated/ground truth\n",
    "    # wd_propagated_gt_trunc = norm_kl_div_pdf2pdf(pdf_truncated_propagated, gt_pdf_trunc, y_vals)\n",
    "    # wd_propagated_gt_higher = norm_kl_div_pdf2pdf(pdf_higher_propagated, gt_pdf_higher, y_vals)\n",
    "\n",
    "    # # network output/propagated\n",
    "    # wd_netout_propagated_trunc = norm_kl_div_histtopdf(network_outputs, pdf_truncated_propagated, y_vals)\n",
    "    # wd_netout_propagated_higher = norm_kl_div_histtopdf(network_outputs, pdf_higher_propagated, y_vals)\n",
    "\n",
    "    # JL predictions/ground truth\n",
    "    wd_jl_gt_trunc = norm_kl_div_pdf2pdf(jl_pdf_trunc, gt_pdf_trunc, y_vals)\n",
    "    wd_jl_gt_higher = norm_kl_div_pdf2pdf(jl_pdf_higher, gt_pdf_higher, y_vals)\n",
    "\n",
    "    # network output/JL predictions\n",
    "    wd_netout_jl_trunc = norm_kl_div_histtopdf(network_outputs, jl_pdf_trunc, y_vals)\n",
    "    wd_netout_jl_higher = norm_kl_div_histtopdf(network_outputs, jl_pdf_higher, y_vals)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region\n",
    "    ####################\n",
    "    # Save to excel file\n",
    "    #####################\n",
    "\n",
    "    def find_next_empty_row(ws, anchor_col=2):\n",
    "        row = 6\n",
    "        while ws.cell(row=row, column=anchor_col).value is not None:\n",
    "            row += 1\n",
    "        return row\n",
    "\n",
    "    # Load your workbook and sheet\n",
    "    # wb = load_workbook(r\"C:\\Users\\cga28\\Documents\\Project\\leaky_relu_uniform.xlsx\")\n",
    "    ws = wb[\"0.5 LR symm\"]  # or the exact name of your sheet\n",
    "\n",
    "    # Example: write raw cumulants starting at column H (8), row 3\n",
    "    # start_row = 12\n",
    "    start_row = find_next_empty_row(ws)\n",
    "    start_col = 2 #(B)\n",
    "    cumulant_start_column = 7\n",
    "    KL_start_column = 27\n",
    "    wass_start_column = 38\n",
    "    dead_start_column = 49\n",
    "    real_number = number + 1\n",
    "\n",
    "    ws.cell(row=start_row, column=start_col, value=real_number)\n",
    "    ws.cell(row=start_row, column=start_col - 1, value=SEED)\n",
    "    ws.cell(row=start_row, column=cumulant_start_column - 1, value=slope)\n",
    "\n",
    "    # ws.cell(row=start_row, column=start_col + 1, value=w0_save)\n",
    "    # ws.cell(row=start_row, column=start_col + 2, value=s2_2a0_w1) # be aware these clash with the slope now\n",
    "\n",
    "    # Write average input cumulants (1st row)\n",
    "    for i, val in enumerate(avg_input_c.values()):\n",
    "        ws.cell(row=start_row, column=cumulant_start_column + i, value=float(val))\n",
    "\n",
    "    # Write JL prediction cumulants\n",
    "    for i, val in enumerate(avg_jl_cumulants.values()):\n",
    "        ws.cell(row=start_row, column=cumulant_start_column + 4 + i, value=float(val))\n",
    "\n",
    "    # Write empirical cumulants\n",
    "    for i, val in enumerate(network_cumulants.values()):\n",
    "        ws.cell(row=start_row, column=cumulant_start_column + 8 + i, value=float(val))\n",
    "\n",
    "    # # Write propagated cumulants\n",
    "    # for i, val in enumerate(cumulants_plotting):\n",
    "    #     ws.cell(row=start_row, column=cumulant_start_column + 12 + i, value=float(val))\n",
    "\n",
    "    # Write ground truth cumulants\n",
    "    for i, val in enumerate(ground_truth_c.values()):\n",
    "        ws.cell(row=start_row, column=cumulant_start_column + 16 + i, value=float(val))\n",
    "\n",
    "\n",
    "    ws.cell(row=start_row, column=KL_start_column, value=kl_netout_gt_trunc)\n",
    "    ws.cell(row=start_row, column=KL_start_column+1, value=kl_netout_gt_higher)\n",
    "\n",
    "    # ws.cell(row=start_row, column=KL_start_column+2, value=kl_propagated_gt_trunc)\n",
    "    # ws.cell(row=start_row, column=KL_start_column+3, value=kl_propagated_gt_higher)\n",
    "\n",
    "    # ws.cell(row=start_row, column=KL_start_column+4, value=kl_netout_propagated_trunc)\n",
    "    # ws.cell(row=start_row, column=KL_start_column+5, value=kl_netout_propagated_higher)\n",
    "\n",
    "    ws.cell(row=start_row, column=KL_start_column+6, value=kl_jl_gt_trunc)\n",
    "    ws.cell(row=start_row, column=KL_start_column+7, value=kl_jl_gt_higher)\n",
    "\n",
    "    ws.cell(row=start_row, column=KL_start_column+8, value=kl_netout_jl_trunc)\n",
    "    ws.cell(row=start_row, column=KL_start_column+9, value=kl_netout_jl_higher)\n",
    "\n",
    "\n",
    "\n",
    "    # Write wasserstein distances\n",
    "    ws.cell(row=start_row, column=wass_start_column, value=wd_netout_gt_trunc)\n",
    "    ws.cell(row=start_row, column=wass_start_column+1, value=wd_netout_gt_higher)\n",
    "\n",
    "    # ws.cell(row=start_row, column=wass_start_column+2, value=wd_propagated_gt_trunc)\n",
    "    # ws.cell(row=start_row, column=wass_start_column+3, value=wd_propagated_gt_higher)\n",
    "\n",
    "    # ws.cell(row=start_row, column=wass_start_column+4, value=wd_netout_propagated_trunc)\n",
    "    # ws.cell(row=start_row, column=wass_start_column+5, value=wd_netout_propagated_higher)\n",
    "\n",
    "    ws.cell(row=start_row, column=wass_start_column+6, value=wd_jl_gt_trunc)\n",
    "    ws.cell(row=start_row, column=wass_start_column+7, value=wd_jl_gt_higher)\n",
    "\n",
    "    ws.cell(row=start_row, column=wass_start_column+8, value=wd_netout_jl_trunc)\n",
    "    ws.cell(row=start_row, column=wass_start_column+9, value=wd_netout_jl_higher)\n",
    "\n",
    "\n",
    "    # Write each neuron's zero-output fraction to Excel\n",
    "    for i, frac in enumerate(zero_fractions):\n",
    "        ws.cell(row=start_row, column=dead_start_column + i, value=frac)\n",
    "\n",
    "    # Save total as one extra column\n",
    "    ws.cell(row=start_row, column=dead_start_column + n_neurons, value=total_dead_frac)\n",
    "\n",
    "\n",
    "    # Save the JL expected values (from the inputs alone) after number iterations\n",
    "    for i, val in enumerate(JL_expected_c.values()):\n",
    "        ws.cell(row=start_row, column=dead_start_column + 4 + i, value=float(val))\n",
    "\n",
    "\n",
    "\n",
    "    # Save workbook\n",
    "    wb.save(r\"C:\\Users\\cga28\\Documents\\Project\\leaky_relu_uniform.xlsx\")\n",
    "    # endregion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency with Analytical Solutions\n",
    "\n",
    "Reminder:\n",
    "- 1 linear layer, no biases: $w_{0} = w_{1} = \\frac{1}{\\sqrt{2}}$\n",
    "- 1 linear layer, with biases: same as above, with $b = 0$\n",
    "- 2 linear layers, no biases: $w_{0} = \\frac{\\sqrt{2}}{2a_{0}} - w_{1}$\n",
    "- 2 linear layers, with biases: $b_{2} = -2a_{0}b_{1}, \\; w_{0} = \\frac{1}{a_{0}\\sqrt(2)} - w_{1}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
